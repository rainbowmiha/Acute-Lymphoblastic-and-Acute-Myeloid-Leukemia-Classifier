{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ALL_AML_Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNND66mWV7PpEQASNKIPS4d"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "620SZD6Czc9v"
      },
      "source": [
        "# **Classifying Samples with Acute Myeloid Leukemia and Acute Lymphoblastic Leukemia: SVM vs XGBoost**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2D-zyWlrz7Ic"
      },
      "source": [
        "This project is based on a study published in 1999 by Golub et al - Molecular Classification of Cancer: Class Discovery and Class Prediction by Gene Expression Monitoring. It demonstrated how new cases of cancer could be classified by gene expression monitoring (via DNA microarray) and thus provided a general approach for identifying new cancer classes and assigning tumors to known classes. [1]\n",
        "\n",
        "The data used in this project, also used as a test case in the paper, is gene expression data from patients with acute myeloid leukemia (AML) and acute lymphoblastic leukemia (ALL). The project's purpose is to train a classifier which would diagnose new patients with one of the two conditions.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6Wy9cCD9Ifv"
      },
      "source": [
        "## **Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EayOTHSo9VT4"
      },
      "source": [
        "### **Data Description**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSNrFnwr96pS"
      },
      "source": [
        "As mentioned, the dataset used in this project contains microarray gene expression data of patients with acute myeloid leukemia (AML) and acute lymphoblastic leukemia (ALL). The data used in the paper was originally split into two datasets of 38 and 34 samples, used respectively for training and testing. In addition, we had one more dataset with the labels (ALL or AML) for all samples.\n",
        "The first two columns of the dataset contain gene descriptions.\n",
        "The dataset contains microarray expression data from 7129 genes, where each gene is a row in the dataset.  Intensity values have been re-scaled such that overall intensities for each chip are equivalent. The samples are the columns of the dataset. Next to the column for each sample there was a column of a variable named \"Call\", which took three values - \"A\", \"P\", and \"M\". These stand for \"Absent\", \"Present\", and \"Marginal\". They are based on the signal in the microarray for a gene at hand and they determine a gene's expression in a sample. In order for XGBoost to handle these categorical variables, these were transformed into numbers - 0 for \"A\", 1 for \"P\", and 2 for \"M\". We first trained classifiers only with the numerical variables, but they did not perform very well, and thus we trained the classifiers with the numerical and some of the categorical features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhSmdvYnGzvU"
      },
      "source": [
        "### **Data Loading**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9Hrz0ysIAgf"
      },
      "source": [
        "We now proceed to loading the original data and importing the necessary libraries and we will then focus on the data processing steps that will be necessary in this case.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuMe8mx-L_lL",
        "outputId": "bfd5ecba-d248-4b34-ec91-af2b4dfa5051",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DETu9PRcIPBq"
      },
      "source": [
        "# Import Libraries\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn import svm\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8FYwWEiOxlO"
      },
      "source": [
        "# Load data\n",
        "\n",
        "labels = \"/content/gdrive/My Drive/MLProjects/MolecularCancerClassification/actual.csv\"\n",
        "test_path = \"/content/gdrive/My Drive/MLProjects/MolecularCancerClassification/data_set_ALL_AML_independent.csv\"\n",
        "train_path = \"/content/gdrive/My Drive/MLProjects/MolecularCancerClassification/data_set_ALL_AML_train.csv\"\n",
        "\n",
        "# replace labels with numbers\n",
        "labels_df = pd.read_csv(labels, index_col = 'patient')\n",
        "\n",
        "X_test = pd.read_csv(test_path)\n",
        "X_train = pd.read_csv(train_path)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubRm2qwor4qe"
      },
      "source": [
        "### **Data Processing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYqtEt2hGC-i"
      },
      "source": [
        "There are several data processing steps that need to be conducted.\n",
        "The first one would be to exctract the \"call\" columns mentioned above and turn them into features.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmmbuDTQSMYK"
      },
      "source": [
        "# extract only call columns, in order to process them and potentially use them as features\n",
        "ds_test = [col for col in X_test.columns if 'call' in col]\n",
        "X_test_call = X_test[ds_test]\n",
        "\n",
        "# drop call columns from X_test\n",
        "X_test.drop(ds_test, axis = 1, inplace = True)\n",
        "\n",
        "# extract only call columns, in order to process them and potentially use them as features\n",
        "ds_train = [col for col in X_train.columns if 'call' in col]\n",
        "X_train_call = X_train[ds_train]\n",
        "\n",
        "# drop call columns from X_test\n",
        "X_train.drop(ds_train, axis = 1, inplace = True)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLDbq2zRXQJh",
        "outputId": "979a066f-0934-4d02-a0e4-d48738dc1628",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "# check whether the manipulation above did what was intended\n",
        "X_train_call.head()\n",
        "X_test_call.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>call</th>\n",
              "      <th>call.1</th>\n",
              "      <th>call.2</th>\n",
              "      <th>call.3</th>\n",
              "      <th>call.4</th>\n",
              "      <th>call.5</th>\n",
              "      <th>call.6</th>\n",
              "      <th>call.7</th>\n",
              "      <th>call.8</th>\n",
              "      <th>call.9</th>\n",
              "      <th>call.10</th>\n",
              "      <th>call.11</th>\n",
              "      <th>call.12</th>\n",
              "      <th>call.13</th>\n",
              "      <th>call.14</th>\n",
              "      <th>call.15</th>\n",
              "      <th>call.16</th>\n",
              "      <th>call.17</th>\n",
              "      <th>call.18</th>\n",
              "      <th>call.19</th>\n",
              "      <th>call.20</th>\n",
              "      <th>call.21</th>\n",
              "      <th>call.22</th>\n",
              "      <th>call.23</th>\n",
              "      <th>call.24</th>\n",
              "      <th>call.25</th>\n",
              "      <th>call.26</th>\n",
              "      <th>call.27</th>\n",
              "      <th>call.28</th>\n",
              "      <th>call.29</th>\n",
              "      <th>call.30</th>\n",
              "      <th>call.31</th>\n",
              "      <th>call.32</th>\n",
              "      <th>call.33</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>P</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  call call.1 call.2 call.3 call.4  ... call.29 call.30 call.31 call.32 call.33\n",
              "0    A      A      A      A      A  ...       A       A       A       A       A\n",
              "1    A      A      A      A      A  ...       A       A       A       A       A\n",
              "2    A      A      A      A      A  ...       A       A       A       A       A\n",
              "3    A      A      A      A      A  ...       A       A       A       A       A\n",
              "4    A      A      A      A      A  ...       A       A       A       A       A\n",
              "\n",
              "[5 rows x 34 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWeNJSQc3ezN"
      },
      "source": [
        "# transform the string categories into numbers\n",
        "X_train_call = X_train_call.replace(\"A\", 0)\n",
        "X_train_call = X_train_call.replace(\"P\", 1)\n",
        "X_train_call = X_train_call.replace(\"M\", 2)\n",
        "\n",
        "X_test_call = X_test_call.replace(\"A\", 0)\n",
        "X_test_call = X_test_call.replace(\"P\", 1)\n",
        "X_test_call = X_test_call.replace(\"M\", 2)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOVOlnDqA53L"
      },
      "source": [
        "# add rownames which include the \"Gene Accession number\" + \"Call\"\n",
        "rownames = \"Call \" + X_train.iloc[:,1]\n",
        "X_train_call.index = rownames\n",
        "X_test_call.index = rownames\n",
        "\n",
        "# extract ids for patients\n",
        "ids_train = list(X_train.columns[2:])\n",
        "ids_test = list(X_test.columns[2:])\n",
        "\n",
        "X_train_call.columns = ids_train\n",
        "X_test_call.columns = ids_test"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eeBPooGh4AJW",
        "outputId": "a58b98da-a7ca-4bd3-f41a-1ba90f87881d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        }
      },
      "source": [
        "# transpose the \"call\" features matrices and prepare them for merging with the rest of the features\n",
        "X_train_call = X_train_call.T\n",
        "X_train_call.head()\n",
        "\n",
        "X_test_call = X_test_call.T\n",
        "X_test_call.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>Gene Accession Number</th>\n",
              "      <th>Call AFFX-BioB-5_at</th>\n",
              "      <th>Call AFFX-BioB-M_at</th>\n",
              "      <th>Call AFFX-BioB-3_at</th>\n",
              "      <th>Call AFFX-BioC-5_at</th>\n",
              "      <th>Call AFFX-BioC-3_at</th>\n",
              "      <th>Call AFFX-BioDn-5_at</th>\n",
              "      <th>Call AFFX-BioDn-3_at</th>\n",
              "      <th>Call AFFX-CreX-5_at</th>\n",
              "      <th>Call AFFX-CreX-3_at</th>\n",
              "      <th>Call AFFX-BioB-5_st</th>\n",
              "      <th>Call AFFX-BioB-M_st</th>\n",
              "      <th>Call AFFX-BioB-3_st</th>\n",
              "      <th>Call AFFX-BioC-5_st</th>\n",
              "      <th>Call AFFX-BioC-3_st</th>\n",
              "      <th>Call AFFX-BioDn-5_st</th>\n",
              "      <th>Call AFFX-BioDn-3_st</th>\n",
              "      <th>Call AFFX-CreX-5_st</th>\n",
              "      <th>Call AFFX-CreX-3_st</th>\n",
              "      <th>Call hum_alu_at</th>\n",
              "      <th>Call AFFX-DapX-5_at</th>\n",
              "      <th>Call AFFX-DapX-M_at</th>\n",
              "      <th>Call AFFX-DapX-3_at</th>\n",
              "      <th>Call AFFX-LysX-5_at</th>\n",
              "      <th>Call AFFX-LysX-M_at</th>\n",
              "      <th>Call AFFX-LysX-3_at</th>\n",
              "      <th>Call AFFX-PheX-5_at</th>\n",
              "      <th>Call AFFX-PheX-M_at</th>\n",
              "      <th>Call AFFX-PheX-3_at</th>\n",
              "      <th>Call AFFX-ThrX-5_at</th>\n",
              "      <th>Call AFFX-ThrX-M_at</th>\n",
              "      <th>Call AFFX-ThrX-3_at</th>\n",
              "      <th>Call AFFX-TrpnX-5_at</th>\n",
              "      <th>Call AFFX-TrpnX-M_at</th>\n",
              "      <th>Call AFFX-TrpnX-3_at</th>\n",
              "      <th>Call AFFX-HUMISGF3A/M97935_5_at</th>\n",
              "      <th>Call AFFX-HUMISGF3A/M97935_MA_at</th>\n",
              "      <th>Call AFFX-HUMISGF3A/M97935_MB_at</th>\n",
              "      <th>Call AFFX-HUMISGF3A/M97935_3_at</th>\n",
              "      <th>Call AFFX-HUMRGE/M10098_5_at</th>\n",
              "      <th>Call AFFX-HUMRGE/M10098_M_at</th>\n",
              "      <th>...</th>\n",
              "      <th>Call X53065_f_at</th>\n",
              "      <th>Call X64177_f_at</th>\n",
              "      <th>Call X67491_f_at</th>\n",
              "      <th>Call X71345_f_at</th>\n",
              "      <th>Call X97444_f_at</th>\n",
              "      <th>Call Z80780_f_at</th>\n",
              "      <th>Call X00351_f_at</th>\n",
              "      <th>Call X01677_f_at</th>\n",
              "      <th>Call M31667_f_at</th>\n",
              "      <th>Call L41268_f_at</th>\n",
              "      <th>Call X99479_f_at</th>\n",
              "      <th>Call HG658-HT658_f_at</th>\n",
              "      <th>Call M94880_f_at</th>\n",
              "      <th>Call S80905_f_at</th>\n",
              "      <th>Call X03068_f_at</th>\n",
              "      <th>Call Z34822_f_at</th>\n",
              "      <th>Call U87593_f_at</th>\n",
              "      <th>Call U88902_cds1_f_at</th>\n",
              "      <th>Call AC002076_cds2_at</th>\n",
              "      <th>Call D64015_at</th>\n",
              "      <th>Call HG2510-HT2606_at</th>\n",
              "      <th>Call L10717_at</th>\n",
              "      <th>Call L34355_at</th>\n",
              "      <th>Call L78833_cds4_at</th>\n",
              "      <th>Call M13981_at</th>\n",
              "      <th>Call M21064_at</th>\n",
              "      <th>Call M93143_at</th>\n",
              "      <th>Call S78825_at</th>\n",
              "      <th>Call U11863_at</th>\n",
              "      <th>Call U29175_at</th>\n",
              "      <th>Call U48730_at</th>\n",
              "      <th>Call U58516_at</th>\n",
              "      <th>Call U73738_at</th>\n",
              "      <th>Call X06956_at</th>\n",
              "      <th>Call X16699_at</th>\n",
              "      <th>Call X83863_at</th>\n",
              "      <th>Call Z17240_at</th>\n",
              "      <th>Call L49218_f_at</th>\n",
              "      <th>Call M71243_f_at</th>\n",
              "      <th>Call Z78285_f_at</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 7129 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "Gene Accession Number  Call AFFX-BioB-5_at  ...  Call Z78285_f_at\n",
              "39                                       0  ...                 0\n",
              "40                                       0  ...                 0\n",
              "42                                       0  ...                 0\n",
              "47                                       0  ...                 0\n",
              "48                                       0  ...                 0\n",
              "\n",
              "[5 rows x 7129 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEc6O-K8Hnrz"
      },
      "source": [
        "# turn ids from strings to integers\n",
        "train_ids = X_train_call.index\n",
        "train_ids = list(map(int, train_ids))\n",
        "\n",
        "test_ids = X_test_call.index\n",
        "test_ids = list(map(int, test_ids))\n",
        "\n",
        "train_labels = []\n",
        "test_labels = []\n",
        "\n",
        "for i in train_ids:\n",
        "  train_labels.append(labels_df.iloc[i-1,0])\n",
        "\n",
        "for j in test_ids:\n",
        "  test_labels.append(labels_df.iloc[j-1,0])\n",
        "\n",
        "\n",
        "train_labels = pd.concat([pd.DataFrame(train_ids), pd.DataFrame(train_labels)], axis=1, ignore_index = False)\n",
        "train_labels.columns = [\"patient\", \"cancer\"]\n",
        "\n",
        "test_labels = pd.concat([pd.DataFrame(test_ids), pd.DataFrame(test_labels)], axis=1, ignore_index = False)\n",
        "test_labels.columns = [\"patient\", \"cancer\"]\n",
        "\n",
        "# extract only the labels in the y variables\n",
        "\n",
        "y_train = train_labels.iloc[:,1]\n",
        "y_test = test_labels.iloc[:,1]\n",
        "\n",
        "# replace ALL labels with 0 and AML labels with 1\n",
        "\n",
        "y_train = y_train.replace(\"ALL\", 0)\n",
        "y_train = y_train.replace(\"AML\", 1)\n",
        "y_train = list(y_train)\n",
        "\n",
        "\n",
        "y_test = y_test.replace(\"ALL\", 0)\n",
        "y_test = y_test.replace(\"AML\", 1)\n",
        "y_test = list(y_test)\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_sfAqMjPMxA",
        "outputId": "50717885-8abc-4232-ebd0-874b4ec8cb48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "source": [
        "# processing the numerical features of the train and test sets\n",
        "gene_names = X_train.iloc[:,1]\n",
        "X_train = X_train.iloc[:,2:]\n",
        "X_train = X_train.T\n",
        "X_train.columns = gene_names\n",
        "X_train.index = train_ids\n",
        "\n",
        "X_test = X_test.iloc[:,2:]\n",
        "X_test = X_test.T\n",
        "X_test.columns = gene_names\n",
        "X_test.index = test_ids\n",
        "X_test.head()\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>Gene Accession Number</th>\n",
              "      <th>AFFX-BioB-5_at</th>\n",
              "      <th>AFFX-BioB-M_at</th>\n",
              "      <th>AFFX-BioB-3_at</th>\n",
              "      <th>AFFX-BioC-5_at</th>\n",
              "      <th>AFFX-BioC-3_at</th>\n",
              "      <th>AFFX-BioDn-5_at</th>\n",
              "      <th>AFFX-BioDn-3_at</th>\n",
              "      <th>AFFX-CreX-5_at</th>\n",
              "      <th>AFFX-CreX-3_at</th>\n",
              "      <th>AFFX-BioB-5_st</th>\n",
              "      <th>AFFX-BioB-M_st</th>\n",
              "      <th>AFFX-BioB-3_st</th>\n",
              "      <th>AFFX-BioC-5_st</th>\n",
              "      <th>AFFX-BioC-3_st</th>\n",
              "      <th>AFFX-BioDn-5_st</th>\n",
              "      <th>AFFX-BioDn-3_st</th>\n",
              "      <th>AFFX-CreX-5_st</th>\n",
              "      <th>AFFX-CreX-3_st</th>\n",
              "      <th>hum_alu_at</th>\n",
              "      <th>AFFX-DapX-5_at</th>\n",
              "      <th>AFFX-DapX-M_at</th>\n",
              "      <th>AFFX-DapX-3_at</th>\n",
              "      <th>AFFX-LysX-5_at</th>\n",
              "      <th>AFFX-LysX-M_at</th>\n",
              "      <th>AFFX-LysX-3_at</th>\n",
              "      <th>AFFX-PheX-5_at</th>\n",
              "      <th>AFFX-PheX-M_at</th>\n",
              "      <th>AFFX-PheX-3_at</th>\n",
              "      <th>AFFX-ThrX-5_at</th>\n",
              "      <th>AFFX-ThrX-M_at</th>\n",
              "      <th>AFFX-ThrX-3_at</th>\n",
              "      <th>AFFX-TrpnX-5_at</th>\n",
              "      <th>AFFX-TrpnX-M_at</th>\n",
              "      <th>AFFX-TrpnX-3_at</th>\n",
              "      <th>AFFX-HUMISGF3A/M97935_5_at</th>\n",
              "      <th>AFFX-HUMISGF3A/M97935_MA_at</th>\n",
              "      <th>AFFX-HUMISGF3A/M97935_MB_at</th>\n",
              "      <th>AFFX-HUMISGF3A/M97935_3_at</th>\n",
              "      <th>AFFX-HUMRGE/M10098_5_at</th>\n",
              "      <th>AFFX-HUMRGE/M10098_M_at</th>\n",
              "      <th>...</th>\n",
              "      <th>X53065_f_at</th>\n",
              "      <th>X64177_f_at</th>\n",
              "      <th>X67491_f_at</th>\n",
              "      <th>X71345_f_at</th>\n",
              "      <th>X97444_f_at</th>\n",
              "      <th>Z80780_f_at</th>\n",
              "      <th>X00351_f_at</th>\n",
              "      <th>X01677_f_at</th>\n",
              "      <th>M31667_f_at</th>\n",
              "      <th>L41268_f_at</th>\n",
              "      <th>X99479_f_at</th>\n",
              "      <th>HG658-HT658_f_at</th>\n",
              "      <th>M94880_f_at</th>\n",
              "      <th>S80905_f_at</th>\n",
              "      <th>X03068_f_at</th>\n",
              "      <th>Z34822_f_at</th>\n",
              "      <th>U87593_f_at</th>\n",
              "      <th>U88902_cds1_f_at</th>\n",
              "      <th>AC002076_cds2_at</th>\n",
              "      <th>D64015_at</th>\n",
              "      <th>HG2510-HT2606_at</th>\n",
              "      <th>L10717_at</th>\n",
              "      <th>L34355_at</th>\n",
              "      <th>L78833_cds4_at</th>\n",
              "      <th>M13981_at</th>\n",
              "      <th>M21064_at</th>\n",
              "      <th>M93143_at</th>\n",
              "      <th>S78825_at</th>\n",
              "      <th>U11863_at</th>\n",
              "      <th>U29175_at</th>\n",
              "      <th>U48730_at</th>\n",
              "      <th>U58516_at</th>\n",
              "      <th>U73738_at</th>\n",
              "      <th>X06956_at</th>\n",
              "      <th>X16699_at</th>\n",
              "      <th>X83863_at</th>\n",
              "      <th>Z17240_at</th>\n",
              "      <th>L49218_f_at</th>\n",
              "      <th>M71243_f_at</th>\n",
              "      <th>Z78285_f_at</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>-342</td>\n",
              "      <td>-200</td>\n",
              "      <td>41</td>\n",
              "      <td>328</td>\n",
              "      <td>-224</td>\n",
              "      <td>-427</td>\n",
              "      <td>-656</td>\n",
              "      <td>-292</td>\n",
              "      <td>137</td>\n",
              "      <td>-144</td>\n",
              "      <td>48</td>\n",
              "      <td>-591</td>\n",
              "      <td>-622</td>\n",
              "      <td>-342</td>\n",
              "      <td>294</td>\n",
              "      <td>241</td>\n",
              "      <td>-7</td>\n",
              "      <td>-108</td>\n",
              "      <td>45815</td>\n",
              "      <td>57</td>\n",
              "      <td>422</td>\n",
              "      <td>-185</td>\n",
              "      <td>-48</td>\n",
              "      <td>-181</td>\n",
              "      <td>-4</td>\n",
              "      <td>-132</td>\n",
              "      <td>-2</td>\n",
              "      <td>115</td>\n",
              "      <td>41</td>\n",
              "      <td>-50</td>\n",
              "      <td>-202</td>\n",
              "      <td>113</td>\n",
              "      <td>-557</td>\n",
              "      <td>-687</td>\n",
              "      <td>-289</td>\n",
              "      <td>-195</td>\n",
              "      <td>135</td>\n",
              "      <td>267</td>\n",
              "      <td>57</td>\n",
              "      <td>-238</td>\n",
              "      <td>...</td>\n",
              "      <td>429</td>\n",
              "      <td>-605</td>\n",
              "      <td>-2</td>\n",
              "      <td>603</td>\n",
              "      <td>381</td>\n",
              "      <td>2435</td>\n",
              "      <td>20818</td>\n",
              "      <td>12869</td>\n",
              "      <td>835</td>\n",
              "      <td>388</td>\n",
              "      <td>-118</td>\n",
              "      <td>16456</td>\n",
              "      <td>12103</td>\n",
              "      <td>451</td>\n",
              "      <td>3239</td>\n",
              "      <td>-352</td>\n",
              "      <td>41</td>\n",
              "      <td>547</td>\n",
              "      <td>-50</td>\n",
              "      <td>156</td>\n",
              "      <td>41</td>\n",
              "      <td>19</td>\n",
              "      <td>323</td>\n",
              "      <td>420</td>\n",
              "      <td>231</td>\n",
              "      <td>246</td>\n",
              "      <td>533</td>\n",
              "      <td>-101</td>\n",
              "      <td>-451</td>\n",
              "      <td>2112</td>\n",
              "      <td>277</td>\n",
              "      <td>1023</td>\n",
              "      <td>67</td>\n",
              "      <td>214</td>\n",
              "      <td>-135</td>\n",
              "      <td>1074</td>\n",
              "      <td>475</td>\n",
              "      <td>48</td>\n",
              "      <td>168</td>\n",
              "      <td>-70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>-87</td>\n",
              "      <td>-248</td>\n",
              "      <td>262</td>\n",
              "      <td>295</td>\n",
              "      <td>-226</td>\n",
              "      <td>-493</td>\n",
              "      <td>367</td>\n",
              "      <td>-452</td>\n",
              "      <td>194</td>\n",
              "      <td>162</td>\n",
              "      <td>207</td>\n",
              "      <td>-960</td>\n",
              "      <td>-622</td>\n",
              "      <td>175</td>\n",
              "      <td>171</td>\n",
              "      <td>-71</td>\n",
              "      <td>-163</td>\n",
              "      <td>-445</td>\n",
              "      <td>20958</td>\n",
              "      <td>21</td>\n",
              "      <td>137</td>\n",
              "      <td>-83</td>\n",
              "      <td>30</td>\n",
              "      <td>81</td>\n",
              "      <td>300</td>\n",
              "      <td>-76</td>\n",
              "      <td>-234</td>\n",
              "      <td>-49</td>\n",
              "      <td>24</td>\n",
              "      <td>-52</td>\n",
              "      <td>-370</td>\n",
              "      <td>-35</td>\n",
              "      <td>-616</td>\n",
              "      <td>-409</td>\n",
              "      <td>-417</td>\n",
              "      <td>-589</td>\n",
              "      <td>96</td>\n",
              "      <td>373</td>\n",
              "      <td>-175</td>\n",
              "      <td>-304</td>\n",
              "      <td>...</td>\n",
              "      <td>111</td>\n",
              "      <td>-644</td>\n",
              "      <td>117</td>\n",
              "      <td>-96</td>\n",
              "      <td>288</td>\n",
              "      <td>-23</td>\n",
              "      <td>9499</td>\n",
              "      <td>17954</td>\n",
              "      <td>569</td>\n",
              "      <td>276</td>\n",
              "      <td>-229</td>\n",
              "      <td>18764</td>\n",
              "      <td>10349</td>\n",
              "      <td>204</td>\n",
              "      <td>9883</td>\n",
              "      <td>-304</td>\n",
              "      <td>-109</td>\n",
              "      <td>383</td>\n",
              "      <td>-13</td>\n",
              "      <td>303</td>\n",
              "      <td>117</td>\n",
              "      <td>87</td>\n",
              "      <td>-142</td>\n",
              "      <td>212</td>\n",
              "      <td>5</td>\n",
              "      <td>325</td>\n",
              "      <td>80</td>\n",
              "      <td>-518</td>\n",
              "      <td>-2406</td>\n",
              "      <td>1323</td>\n",
              "      <td>83</td>\n",
              "      <td>529</td>\n",
              "      <td>-295</td>\n",
              "      <td>352</td>\n",
              "      <td>-67</td>\n",
              "      <td>67</td>\n",
              "      <td>263</td>\n",
              "      <td>-33</td>\n",
              "      <td>-33</td>\n",
              "      <td>-21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>22</td>\n",
              "      <td>-153</td>\n",
              "      <td>17</td>\n",
              "      <td>276</td>\n",
              "      <td>-211</td>\n",
              "      <td>-250</td>\n",
              "      <td>55</td>\n",
              "      <td>-141</td>\n",
              "      <td>0</td>\n",
              "      <td>500</td>\n",
              "      <td>58</td>\n",
              "      <td>-517</td>\n",
              "      <td>-24</td>\n",
              "      <td>-99</td>\n",
              "      <td>13</td>\n",
              "      <td>-77</td>\n",
              "      <td>-182</td>\n",
              "      <td>-150</td>\n",
              "      <td>25880</td>\n",
              "      <td>41</td>\n",
              "      <td>70</td>\n",
              "      <td>-12</td>\n",
              "      <td>37</td>\n",
              "      <td>-15</td>\n",
              "      <td>49</td>\n",
              "      <td>-52</td>\n",
              "      <td>-120</td>\n",
              "      <td>-23</td>\n",
              "      <td>-37</td>\n",
              "      <td>-70</td>\n",
              "      <td>-254</td>\n",
              "      <td>51</td>\n",
              "      <td>-340</td>\n",
              "      <td>-149</td>\n",
              "      <td>-153</td>\n",
              "      <td>-124</td>\n",
              "      <td>194</td>\n",
              "      <td>532</td>\n",
              "      <td>-55</td>\n",
              "      <td>-341</td>\n",
              "      <td>...</td>\n",
              "      <td>186</td>\n",
              "      <td>-770</td>\n",
              "      <td>478</td>\n",
              "      <td>374</td>\n",
              "      <td>363</td>\n",
              "      <td>547</td>\n",
              "      <td>20485</td>\n",
              "      <td>17102</td>\n",
              "      <td>1383</td>\n",
              "      <td>96</td>\n",
              "      <td>189</td>\n",
              "      <td>16180</td>\n",
              "      <td>19089</td>\n",
              "      <td>6</td>\n",
              "      <td>1038</td>\n",
              "      <td>-131</td>\n",
              "      <td>37</td>\n",
              "      <td>355</td>\n",
              "      <td>15</td>\n",
              "      <td>179</td>\n",
              "      <td>87</td>\n",
              "      <td>77</td>\n",
              "      <td>192</td>\n",
              "      <td>235</td>\n",
              "      <td>95</td>\n",
              "      <td>189</td>\n",
              "      <td>243</td>\n",
              "      <td>-189</td>\n",
              "      <td>-3710</td>\n",
              "      <td>826</td>\n",
              "      <td>413</td>\n",
              "      <td>399</td>\n",
              "      <td>16</td>\n",
              "      <td>558</td>\n",
              "      <td>24</td>\n",
              "      <td>893</td>\n",
              "      <td>297</td>\n",
              "      <td>6</td>\n",
              "      <td>1971</td>\n",
              "      <td>-42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>-243</td>\n",
              "      <td>-218</td>\n",
              "      <td>-163</td>\n",
              "      <td>182</td>\n",
              "      <td>-289</td>\n",
              "      <td>-268</td>\n",
              "      <td>-285</td>\n",
              "      <td>-172</td>\n",
              "      <td>52</td>\n",
              "      <td>-134</td>\n",
              "      <td>-140</td>\n",
              "      <td>-321</td>\n",
              "      <td>-439</td>\n",
              "      <td>-149</td>\n",
              "      <td>-56</td>\n",
              "      <td>117</td>\n",
              "      <td>-31</td>\n",
              "      <td>-243</td>\n",
              "      <td>41275</td>\n",
              "      <td>84</td>\n",
              "      <td>178</td>\n",
              "      <td>-172</td>\n",
              "      <td>-14</td>\n",
              "      <td>-271</td>\n",
              "      <td>73</td>\n",
              "      <td>18</td>\n",
              "      <td>-2</td>\n",
              "      <td>16</td>\n",
              "      <td>-23</td>\n",
              "      <td>102</td>\n",
              "      <td>18</td>\n",
              "      <td>8</td>\n",
              "      <td>-462</td>\n",
              "      <td>-212</td>\n",
              "      <td>-191</td>\n",
              "      <td>52</td>\n",
              "      <td>203</td>\n",
              "      <td>1466</td>\n",
              "      <td>-18</td>\n",
              "      <td>-189</td>\n",
              "      <td>...</td>\n",
              "      <td>172</td>\n",
              "      <td>-224</td>\n",
              "      <td>58</td>\n",
              "      <td>271</td>\n",
              "      <td>205</td>\n",
              "      <td>317</td>\n",
              "      <td>11122</td>\n",
              "      <td>17048</td>\n",
              "      <td>199</td>\n",
              "      <td>147</td>\n",
              "      <td>27</td>\n",
              "      <td>10313</td>\n",
              "      <td>8428</td>\n",
              "      <td>199</td>\n",
              "      <td>3242</td>\n",
              "      <td>-69</td>\n",
              "      <td>8</td>\n",
              "      <td>121</td>\n",
              "      <td>-39</td>\n",
              "      <td>18</td>\n",
              "      <td>-54</td>\n",
              "      <td>-6</td>\n",
              "      <td>-197</td>\n",
              "      <td>262</td>\n",
              "      <td>-16</td>\n",
              "      <td>23</td>\n",
              "      <td>168</td>\n",
              "      <td>-180</td>\n",
              "      <td>-1460</td>\n",
              "      <td>613</td>\n",
              "      <td>174</td>\n",
              "      <td>277</td>\n",
              "      <td>6</td>\n",
              "      <td>81</td>\n",
              "      <td>2</td>\n",
              "      <td>722</td>\n",
              "      <td>170</td>\n",
              "      <td>0</td>\n",
              "      <td>510</td>\n",
              "      <td>-73</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>-130</td>\n",
              "      <td>-177</td>\n",
              "      <td>-28</td>\n",
              "      <td>266</td>\n",
              "      <td>-170</td>\n",
              "      <td>-326</td>\n",
              "      <td>-222</td>\n",
              "      <td>-93</td>\n",
              "      <td>10</td>\n",
              "      <td>159</td>\n",
              "      <td>-3</td>\n",
              "      <td>-643</td>\n",
              "      <td>-502</td>\n",
              "      <td>-129</td>\n",
              "      <td>-31</td>\n",
              "      <td>247</td>\n",
              "      <td>-100</td>\n",
              "      <td>-498</td>\n",
              "      <td>11195</td>\n",
              "      <td>-20</td>\n",
              "      <td>188</td>\n",
              "      <td>-75</td>\n",
              "      <td>-29</td>\n",
              "      <td>-19</td>\n",
              "      <td>150</td>\n",
              "      <td>-119</td>\n",
              "      <td>-82</td>\n",
              "      <td>-63</td>\n",
              "      <td>-44</td>\n",
              "      <td>-24</td>\n",
              "      <td>-232</td>\n",
              "      <td>31</td>\n",
              "      <td>-701</td>\n",
              "      <td>-366</td>\n",
              "      <td>-413</td>\n",
              "      <td>-437</td>\n",
              "      <td>54</td>\n",
              "      <td>650</td>\n",
              "      <td>1453</td>\n",
              "      <td>632</td>\n",
              "      <td>...</td>\n",
              "      <td>114</td>\n",
              "      <td>-541</td>\n",
              "      <td>232</td>\n",
              "      <td>290</td>\n",
              "      <td>322</td>\n",
              "      <td>742</td>\n",
              "      <td>17100</td>\n",
              "      <td>12692</td>\n",
              "      <td>210</td>\n",
              "      <td>226</td>\n",
              "      <td>208</td>\n",
              "      <td>14833</td>\n",
              "      <td>12933</td>\n",
              "      <td>530</td>\n",
              "      <td>1350</td>\n",
              "      <td>-124</td>\n",
              "      <td>7</td>\n",
              "      <td>269</td>\n",
              "      <td>2</td>\n",
              "      <td>200</td>\n",
              "      <td>69</td>\n",
              "      <td>63</td>\n",
              "      <td>147</td>\n",
              "      <td>285</td>\n",
              "      <td>-10</td>\n",
              "      <td>296</td>\n",
              "      <td>413</td>\n",
              "      <td>-146</td>\n",
              "      <td>-677</td>\n",
              "      <td>1475</td>\n",
              "      <td>233</td>\n",
              "      <td>643</td>\n",
              "      <td>51</td>\n",
              "      <td>450</td>\n",
              "      <td>-46</td>\n",
              "      <td>612</td>\n",
              "      <td>370</td>\n",
              "      <td>29</td>\n",
              "      <td>333</td>\n",
              "      <td>-19</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 7129 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "Gene Accession Number  AFFX-BioB-5_at  AFFX-BioB-M_at  ...  M71243_f_at  Z78285_f_at\n",
              "39                               -342            -200  ...          168          -70\n",
              "40                                -87            -248  ...          -33          -21\n",
              "42                                 22            -153  ...         1971          -42\n",
              "47                               -243            -218  ...          510          -73\n",
              "48                               -130            -177  ...          333          -19\n",
              "\n",
              "[5 rows x 7129 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvRaOv8DsG1P"
      },
      "source": [
        "Having done the processing step, we now proceed with a brief exploratory data analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8GMFUmsrVAG"
      },
      "source": [
        "## **Exploratory Data Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGIAkIbi4IHI"
      },
      "source": [
        "Before fitting a machine learning model, it is important to get to know the data. Determining whether there is an imbalance is an important step of exploratory data analysis, as, if left unchecked, it could then affect the performance of the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSncXr-04n_M"
      },
      "source": [
        "We first check whether there is an imbalance in the train set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yt5pTiICHLup",
        "outputId": "90f14d4a-90ff-4be9-d637-9e116d8a1c5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        }
      },
      "source": [
        "# check numbers of samples in both conditions (ALL and AML), to see if they are balanced\n",
        "sns.set_theme(style=\"darkgrid\")\n",
        "ax = sns.countplot(x=\"cancer\", data=train_labels, order = [\"ALL\", \"AML\"]).set_title(\"Train set Classes Count\")\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No4GYerV4u-a"
      },
      "source": [
        "As one can see, there is a significant imbalance of ALL to AML samples, which could potentially result into training a classifier which does well on classifying ALL, but not AML samples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f91a77LE46NP"
      },
      "source": [
        "Since the test set was predetermined, it is also worth checking whether it is imbalanced, because this can later on affect the evaluation of the classifier fit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TLA9I9sHcn0",
        "outputId": "a7a694c3-ad97-4efa-a909-c3190c8a265b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        }
      },
      "source": [
        "sns.set_theme(style=\"darkgrid\")\n",
        "ax = sns.countplot(x=\"cancer\", data=test_labels, order = [\"ALL\", \"AML\"]).set_title(\"Test set Classes Count\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEcCAYAAADpzeJvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deVRTZ/4G8CcBgyIqQiEE5Kh1Ky04KCjuHhCEsSrW5UDdpii4UlqtFVymjAhWKEOt1rqPjjPWclwpCoJoRxmsWlunLrVqW5GRhB2rIIok9/eHPzOlEIkXSII8n3N6Tu723u+l1zx535vcKxEEQQAREdFzkhq7ACIiapkYIEREJAoDhIiIRGGAEBGRKAwQIiIShQFCRESiMECImtnBgwfx5ptvGrsMoibHACGj6Nevn/a/V155BX379tVOf/nll8/d3owZM7Bv374mr1PfN//s7GxMmzYN/fr1w6BBgzB9+nScOHGiyetpTtXV1diwYQNGjx4Nd3d3+Pj4YNmyZbhz506z7vfcuXMYMWJEs+6Dmoe5sQug1unixYva1z4+PoiNjcWQIUOMWJF4x44dw/Lly7Fs2TJs3rwZ7du3x4ULF/Dll19i1KhRxi5PbxERESgsLERiYiJeffVVVFVV4csvv8TXX3+NKVOmGLs8MkUCkZF5e3sLOTk5giAIglqtFrZs2SKMGjVKGDhwoBARESGUl5cLgiAIDx8+FN577z1h4MCBgoeHhzBx4kShuLhYSEpKEl555RXB1dVVcHd3F1atWlVnH7q2FQRBuHfvnrBs2TJh6NChwrBhw4SkpCShpqZG+OmnnwRXV1fhlVdeEdzd3QUPD4867Wo0GmHkyJHCtm3bdB7fgQMHhODgYO306tWrhREjRgj9+vUT3njjDeGbb77RLvv++++FN954Q+jXr58wePBgYc2aNaLrFwRByM3NFaZNmyb0799fGDhwoPDOO+/UW2NOTo7g5uYmKJVKncdRUFAgzJ07VxgwYIDg6+srJCcna5dFRkYKSUlJ2umzZ88Kw4cP1057e3sL27dvF8aOHSv0799feOedd4SHDx8KlZWVgpubm9CnTx/B3d1dcHd3FwoKCnTWQKaFPRAyKf/4xz+QlZWFf/7zn7CxsUFsbCxiYmKQlJSEQ4cOoaKiAv/6178gk8lw7do1tG3bFosWLcJ3332H8ePH6/ykrGtbAIiKioKtrS0yMzNRVVWFuXPnQqFQIDg4GKtWrcK+ffuwd+/eetv95ZdfoFKp4O/vr/cxurm5YeHChejQoQN2796Nd955BydPnoSFhQXi4uIwc+ZMTJgwAZWVlbh582aj6v/kk08wdOhQ7N69G48fP8bly5frrenMmTPo27cvFAqFzroXL16MXr16ITs7G7/88gtCQkLg7OyMwYMH63Xc6enp2L59OywsLPDmm29qhwe3bduG999/H6dPn9b7b0imgddAyKR88cUXWLRoERwcHCCTyRAeHo6MjAzU1NTA3Nwcd+/exe3bt2FmZgZXV1dYWVnp1a6ubUtKSnDq1CksX74clpaWsLW1xVtvvYWjR4/q1e7du3cBAPb29nofY2BgIDp37gxzc3PMmjUL1dXVuHXrlrbOvLw8lJWVoX379nB3d29U/ebm5lAqlSgqKoKFhQU8PT11HoednZ3OmlUqFb777jssWbIEFhYWcHFxwZQpU5CSkqL3cc+YMQNyuRzW1tbw9vbGtWvX9N6WTBN7IGRSlEolFi5cCKn0f59tpFIpSktLERgYiIKCAixevBj37t3D+PHjsWjRIrRp06bBdnVtq1QqUVNTg2HDhmnX1Wg0z/wk/lvW1tYAgKKiIjg7O+u1zY4dO7B//34UFRVBIpGgoqIC5eXlAIC4uDisX78ef/zjH9GlSxeEh4fD29tbdP3vv/8+PvnkE0yePBmdOnVCSEgIJk+eXO9x5Obm6qy5qKgInTp1qhXYjo6OuHLlil7HDKBWQLVr1w5FRUV6b0umiQFCJsXBwQFr1qyBh4dHvcvDw8MRHh6OO3fuYM6cOejevbteF3jbtGlT77YjR46ETCbD2bNnYW5e95+DRCJ5Zrsvv/wyFAoFMjMzMXv27AbruHDhArZv345du3ahV69ekEqlGDBgAIT/vyl2t27dkJSUBI1Gg8zMTERERODcuXOwtLQUVb+dnR1iY2O1+w4JCcGAAQPQtWvXWusNGTIEu3fvRkFBARwcHOq0Y29vj19//RUVFRXaEFGpVJDL5QCeBMLDhw+165eUlDT4t3iqob8xmS4OYZFJefPNN7Fu3Trk5+cDAMrKypCVlQUAOHv2LK5fvw61Wg0rKyuYm5treyovvfQS/vvf/+psV9e29vb2GDp0KNauXYuKigpoNBrk5eXh/PnzAABbW1sUFhaiurq63nYlEgmioqLw2Wef4cCBA9o2Lly4gD//+c911q+srISZmRlsbGxQU1ODTz/9FBUVFdrlKSkpKCsrg1QqRceOHQE86YGJrT89PR0FBQUAgE6dOkEikdTq3T01ZMgQDBkyBAsXLsSVK1dQU1ODiooK7N27F/v374dCoUC/fv2QlJSER48e4ccff8T+/fsxfvx4AICLiwtOnTqFu3fvori4GH//+9+f8X+5NltbW9y9exf379/XexsyDeyBkEmZOXMmBEHArFmzUFRUBFtbW4wZMwa+vr4oKSlBdHQ0CgsLYWlpiTFjxiAwMFC7XVRUFPbu3YvAwECsXLmyVrvP2jYhIQGJiYkYM2YMKisr4ezsjLCwMADAoEGD0LNnTwwbNgwSiQTnzp2rU3NAQAAsLS2xefNmxMbGwsLCAr169aq3RzJs2DAMHz4c/v7+sLS0xJ/+9Kdaw2XZ2dlYu3YtHj58CEdHR3z88cdo27at6PovX76MNWvWoKKiAra2tlixYoXOobb169dj8+bNWLRoEYqLi9G5c2dtqABAUlISoqOjMXz4cHTs2BFvv/229qvXgYGBOHPmDHx8fODk5IRJkybhb3/7m17/z3v06IHXX38dvr6+UKvVOHr0qLZnQ6ZNIgh8oBQRET0/DmEREZEoDBAiIhKFAUJERKIwQIiISBQGCBERicIAISIiUVrV70DKyyuh0fBby0RE+pBKJejcub3O5a0qQDQagQFCRNREOIRFRESiMECIiEgUBggREYnCACEiIlEMEiDl5eUICwuDv78/xo0bh/DwcJSVlQEA/vOf/2D8+PHw9/fHrFmzUFpaWm8bVVVVePfdd+Hn54eAgAB89dVXhiidiIh0MEiASCQShIaGIiMjA6mpqXB2dkZiYiI0Gg3ef/99fPDBB8jIyICnpycSExPrbWPHjh2wsrLC8ePHsXnzZqxcuRKVlZWGKJ+IiOphkACxtraGl5eXdtrd3R1KpRJXrlyp9Zzm4OBgHDt2rN420tPTERQUBODJU9tcXV1x+vTp5i+eiIjqZfDfgWg0Guzduxc+Pj5QqVRwdHTULrOxsYFGo8Hdu3e1z5p+SqlUwsnJSTutUCi0T1rTl62tVcMrPUP1YzVkbcwa1Qa9eHheUGtl8ABZvXo1LC0tMX36dBw/ftyg+y4trWjUDwnt7Dpg6tI9TVgRvQg+T5iG4mI+jpVePFKp5JkfvA36Laz4+Hjcvn0b69atg1QqhUKhgFKp1C5/+izo3/c+AMDR0VH7nGwAUKlUcHBwMEjdRERUl8ECJCkpCVeuXMHGjRshk8kAAK6urnj48CEuXLgAAPjiiy8QEBBQ7/YBAQFITk4GAOTm5uLy5csYPny4YYonIqI6DDKEdfPmTWzZsgXdunVDcHAwAKBLly7YuHEjEhISEB0djUePHsHJyQkfffSRdrvAwEBs3boVcrkcs2fPRlRUFPz8/CCVShETEwMrq8Zd0yAiIvEkgiC0mrsL8hoINQdeA6EXlUldAyEiohcHA4SIiERhgBARkSgMECIiEoUBQkREojBAiIhIFAYIERGJwgAhIiJRGCBERCQKA4SIiERhgBARkSgMECIiEoUBQkREojBAiIhIFAYIERGJwgAhIiJRDPJEQuDJ89AzMjKQn5+P1NRU9O7dG3fu3MHChQu169y/fx8VFRU4f/58ne03bNiAzz//HPb29gCA/v37Izo62lDlExHR7xgsQEaNGoWZM2di2rRp2nldunRBSkqKdjouLg5qtVpnGxMmTEBkZGSz1klERPoxWIB4eno+c3l1dTVSU1OxY8cOA1VERESNYTLXQE6ePAm5XI7XXntN5zpHjx7FuHHjMGvWLFy8eNGA1RER0e8ZrAfSkAMHDmDSpEk6lwcHB2PevHlo06YNcnJysGDBAqSlpaFz58567+NZD4cnagw7uw7GLoHI4EwiQAoLC/HNN98gISFB5zp2dnba10OHDoVCocDNmzcxcOBAvfdTWloBjUYQXSffJEiX4uL7xi6BqMlJpZJnfvA2iSGsQ4cOYeTIkc/sTRQWFmpfX7t2Dfn5+ejevbshyiMionoYrAcSGxuLzMxMlJSUICQkBNbW1jh69CiAJwGyYsWKOtuEhYUhIiICbm5uSEpKwtWrVyGVStGmTRskJCTU6pUQEZFhSQRBED+m08I0xRDW1KV7mrAiehF8njCNQ1j0QmoRQ1hERNTyMECIiEgUBggREYnCACEiIlEYIEREJAoDhIiIRGGAEBGRKAwQIiIShQFCRESiMECIiEgUBggREYnCACEiIlEYIEREJAoDhIiIRGGAEBGRKAwQIiIShQFCRESiGCxA4uPj4ePjgz59+uDGjRva+T4+PggICEBgYCACAwORnZ1d7/ZVVVV499134efnh4CAAHz11VeGKp2IiOphsGeijxo1CjNnzsS0adPqLFu/fj169+79zO137NgBKysrHD9+HLm5uZg2bRoyMzPRvn375iqZiIiewWA9EE9PTygUCtHbp6enIygoCADQrVs3uLq64vTp001VHhERPSeD9UCeZcmSJRAEAR4eHli8eDE6duxYZx2lUgknJyfttEKhQEFBgSHLJCKi3zB6gOzZswcKhQLV1dWIi4tDTEwMEhMTm2VftrZWzdIukZ1dB2OXQGRwRg+Qp8NaMpkMU6dOxfz58+tdz9HREfn5+bCxsQEAqFQqeHl5Pde+SksroNEIomvlmwTpUlx839glEDU5qVTyzA/eRv0a74MHD3D//pN/eIIgIC0tDS4uLvWuGxAQgOTkZABAbm4uLl++jOHDhxusViIiqs1gARIbG4sRI0agoKAAISEheP3111FaWooZM2Zg3LhxGDt2LG7duoXo6GjtNoGBgSgsLAQAzJ49G/fu3YOfnx/mzp2LmJgYWFlxSIqIyFgkgiCIH9NpYZpiCGvq0j1NWBG9CD5PmMYhLHohmfQQFhERtVwMECIiEoUBQkREojBAiIhIFAYIERGJYvQfEhJR43XuJIO5zMLYZZCJqal+hPJfq5utfQYI0QvAXGaBbxNCjV0GmRiPpdsBNF+AcAiLiIhEYYAQEZEoDBAiIhKFAUJERKIwQIiISBQGCBERicIAISIiURggREQkCgOEiIhEYYAQEZEoBruVSXx8PDIyMpCfn4/U1FT07t0b5eXlWLp0KfLy8iCTydC1a1fExMTAxsamzvZRUVE4c+YMOnfuDODJM9Lnz59vqPKJiOh3DNYDGTVqFPbs2QMnJyftPIlEgtDQUGRkZCA1NRXOzs5ITEzU2cacOXOQkpKClJQUhgcRkZEZLEA8PT2hUChqzbO2toaXl5d22t3dHUql0lAlERFRI5jMNRCNRoO9e/fCx8dH5zo7d+7EuHHjsGDBAvz8888GrI6IiH7PZG7nvnr1alhaWmL69On1Ll+0aBHs7OwglUpx+PBhhIaGIisrC2ZmZnrvw9bWqqnKJarFzq6DsUsgqldznpsmESDx8fG4ffs2Nm/eDKm0/k6RXC7Xvp4wYQI+/PBDFBQU1Lqm0pDS0gpoNILoOvkmQboUF9836v55bpIujTk3pVLJMz94G30IKykpCVeuXMHGjRshk8l0rldYWKh9nZ2dDalUWitUiIjIsAzWA4mNjUVmZiZKSkoQEhICa2trrFu3Dlu2bEG3bt0QHBwMAOjSpQs2btwIAAgMDMTWrVshl8sRGRmJ0tJSSCQSWFlZYdOmTTA3N4kOFBFRq2Swd+CVK1di5cqVdeZfv35d5zYpKSna17t27WqOsoiISCSjD2EREVHLpHeA7Nixo975O3fubLJiiIio5dA7QJ5el/i9TZs2NVkxRETUcjR4DeTrr78G8OSHfmfPnoUg/O9rsHfu3EH79u2brzoiIjJZDQbIihUrAACPHj3C8uXLtfMlEgns7OzqvTBOREQvvgYD5OTJkwCApUuXIiEhodkLIiKilkHvr/H+Njw0Gk2tZbp+PU5ERC8uvQPk6tWriImJwfXr1/Ho0SMAgCAIkEgkuHbtWrMVSEREpknvAImKioK3tzfWrFmDtm3bNmdNRETUAugdIPn5+Vi0aBEkEklz1kNERC2E3hcv/Pz88O9//7s5ayEiohZE7x7Io0ePEB4eDg8PD7z00ku1lvHbWURErY/eAdKzZ0/07NmzOWshIqIWRO8ACQ8Pb846iIiohdE7QJ7e0qQ+gwcPbpJiiIio5dA7QJ7e0uSp8vJyPH78GHK5HCdOnGjywoiIyLTpHSBPb2nylFqtxqZNm3gzRSKiVkr0PUjMzMwwb948bN++vcF14+Pj4ePjgz59+uDGjRva+bdu3UJQUBD8/f0RFBSE3NzcerdXq9VYtWoVfH194efnh3379oktm4iImkijbmKVk5Oj1w8LR40ahT179sDJyanW/OjoaEydOhUZGRmYOnUqPvjgg3q3T01NRV5eHjIzM5GcnIwNGzbgzp07jSmdiIgaSe8hrJEjR9YKi6qqKlRXVyM6OrrBbT09PevMKy0txQ8//KB9ouHYsWOxevVqlJWVwcbGpta6aWlpmDJlCqRSKWxsbODr64tjx44hNDRU3/KJiKiJ6R0gH330Ua3pdu3aoXv37rCyshK1Y5VKBblcDjMzMwBPhsTs7e2hUqnqBIhKpYKjo6N2WqFQoKCgQNR+iYioaegdIAMHDgTw5FbuJSUleOmll1rcbdxtbcWFHVFD7Ow6GLsEono157mpd4BUVFQgJiYGaWlpqKmpgbm5OV5//XWsXLkSHTo8f4EKhQKFhYVQq9UwMzODWq1GUVERFApFvesqlUr07dsXQN0eib5KSyug0QgNr6gD3yRIl+Li+0bdP89N0qUx56ZUKnnmB2+9uxCxsbGoqqpCamoqLl26hNTUVFRVVSE2NlZUYba2tnBxccGRI0cAAEeOHIGLi0ud4SsACAgIwL59+6DRaFBWVoasrCz4+/uL2i8RETUNvXsg2dnZyMrKQrt27QAA3bt3x4cffgg/P78Gt42NjUVmZiZKSkoQEhICa2trHD16FH/5y18QFRWFzz77DB07dkR8fLx2m7CwMERERMDNzQ2BgYH4/vvvMXr0aADAwoUL4ezs/LzHSkRETUjvALGwsEBZWVmtr+KWl5dDJpM1uO3KlSuxcuXKOvN79Oih8zcd27Zt0742MzPDqlWr9C2ViIgMQO8AmTx5MmbNmoW33noLjo6OUCqV2LVrF6ZMmdKc9RERkYnSO0Dmz58PuVyO1NRUFBUVwd7eHqGhoQwQIqJWSu+L6HFxcejevTt27dqFtLQ07Nq1Cz169EBcXFxz1kdERCZK7wA5cuQIXF1da81zdXXVfouKiIhaF70DRCKRQKPR1JqnVqvrzCMiotZB7wDx9PTEJ598og0MjUaDDRs21HufKyIievE91wOl5s6di2HDhsHR0REqlQp2dnbYvHlzc9ZHREQmSu8AcXBwwKFDh3Dp0iWoVCooFAr07du3xd0Pi4iImobeAQIAUqkU7u7ucHd3b656iIiohWD3gYiIRGGAEBGRKAwQIiIShQFCRESiMECIiEgUBggREYnCACEiIlEYIEREJMpz/ZCwOdy5cwcLFy7UTt+/fx8VFRU4f/58rfU2bNiAzz//HPb29gCA/v37Izo62qC1EhHR/xg9QLp06YKUlBTtdFxcHNRqdb3rTpgwAZGRkYYqjYiInsGkhrCqq6uRmpqKSZMmGbsUIiJqgEkFyMmTJyGXy/Haa6/Vu/zo0aMYN24cZs2ahYsXLxq4OiIi+i2jD2H91oEDB3T2PoKDgzFv3jy0adMGOTk5WLBgAdLS0tC5c2e927e1tWqqUolqsbPrYOwSiOrVnOemyQRIYWEhvvnmGyQkJNS73M7OTvt66NChUCgUuHnzJgYOHKj3PkpLK6DRCKJr5JsE6VJcfN+o++e5Sbo05tyUSiXP/OBtMkNYhw4dwsiRI3X2KAoLC7Wvr127hvz8fHTv3t1Q5RER0e+YTA/k0KFDWLFiRa15YWFhiIiIgJubG5KSknD16lVIpVK0adMGCQkJtXolRERkWCYTIBkZGXXmbdu2Tfs6Pj7ekOUQEVEDTGYIi4iIWhYGCBERicIAISIiURggREQkCgOEiIhEYYAQEZEoDBAiIhKFAUJERKIwQIiISBQGCBERicIAISIiURggREQkCgOEiIhEYYAQEZEoDBAiIhKFAUJERKIwQIiISBSTeCKhj48PZDIZLCwsAABLlizB8OHDa61TVVWFZcuW4erVqzAzM0NkZCS8vb2NUS4REcFEAgQA1q9fj969e+tcvmPHDlhZWeH48ePIzc3FtGnTkJmZifbt2xuwSiIieqrFDGGlp6cjKCgIANCtWze4urri9OnTRq6KiKj1MpkeyJIlSyAIAjw8PLB48WJ07Nix1nKlUgknJyfttEKhQEFBgaHLJCKi/2cSAbJnzx4oFApUV1cjLi4OMTExSExMbPL92NpaNXmbRABgZ9fB2CUQ1as5z02TCBCFQgEAkMlkmDp1KubPn19nHUdHR+Tn58PGxgYAoFKp4OXl9Vz7KS2tgEYjiK6TbxKkS3HxfaPun+cm6dKYc1MqlTzzg7fRr4E8ePAA9+8/OUBBEJCWlgYXF5c66wUEBCA5ORkAkJubi8uXL9f5phYRERmO0XsgpaWlePvtt6FWq6HRaNCjRw9ER0cDAAIDA7F161bI5XLMnj0bUVFR8PPzg1QqRUxMDKysOCRFRGQsRg8QZ2dnHD58uN5lKSkp2teWlpZYv369ocoiIqIGGH0Ii4iIWiYGCBERicIAISIiURggREQkCgOEiIhEYYAQEZEoDBAiIhKFAUJERKIwQIiISBQGCBERicIAISIiURggREQkCgOEiIhEYYAQEZEoDBAiIhKFAUJERKIwQIiISBSjP5GwvLwcS5cuRV5eHmQyGbp27YqYmBjY2NjUWi8qKgpnzpxB586dATx5Rvr8+fONUTIREcEEAkQikSA0NBReXl4AgPj4eCQmJmLNmjV11p0zZw6mT59u6BKJiKgeRh/Csra21oYHALi7u0OpVBqxIiIi0ofReyC/pdFosHfvXvj4+NS7fOfOnUhOToazszPee+899OjR47nat7W1aooyieqws+tg7BKI6tWc56ZJBcjq1athaWlZ7zDVokWLYGdnB6lUisOHDyM0NBRZWVkwMzPTu/3S0gpoNILo+vgmQboUF9836v55bpIujTk3pVLJMz94G30I66n4+Hjcvn0b69atg1Ratyy5XK6dP2HCBDx48AAFBQWGLpOIiP6fSQRIUlISrly5go0bN0Imk9W7TmFhofZ1dnY2pFIp5HK5oUokIqLfMfoQ1s2bN7FlyxZ069YNwcHBAIAuXbpg48aNCAwMxNatWyGXyxEZGYnS0lJIJBJYWVlh06ZNMDc3evlERK2W0d+Be/XqhevXr9e7LCUlRft6165dBqqIiIj0YRJDWERE1PIwQIiISBQGCBERicIAISIiURggREQkCgOEiIhEYYAQEZEoDBAiIhKFAUJERKIwQIiISBQGCBERicIAISIiURggREQkCgOEiIhEYYAQEZEoDBAiIhKFAUJERKKYRIDcunULQUFB8Pf3R1BQEHJzc+uso1arsWrVKvj6+sLPzw/79u0zfKFERKRlEgESHR2NqVOnIiMjA1OnTsUHH3xQZ53U1FTk5eUhMzMTycnJ2LBhA+7cuWOEaomICDCBZ6KXlpbihx9+wM6dOwEAY8eOxerVq1FWVgYbGxvtemlpaZgyZQqkUilsbGzg6+uLY8eOITQ0VO99SaWSRtf7Uuf2jW6DXjxNcW41lqyjrbFLIBPUmHOzoW2NHiAqlQpyuRxmZmYAADMzM9jb20OlUtUKEJVKBUdHR+20QqFAQUHBc+2rcxO8+a9fNqHRbdCLx9bWytglwG1evLFLIBPUnOemSQxhERFRy2P0AFEoFCgsLIRarQbw5GJ5UVERFApFnfWUSqV2WqVSwcHBwaC1EhHR/xg9QGxtbeHi4oIjR44AAI4cOQIXF5daw1cAEBAQgH379kGj0aCsrAxZWVnw9/c3RslERARAIgiCYOwifv75Z0RFReHevXvo2LEj4uPj8fLLLyMsLAwRERFwc3ODWq1GTEwMcnJyAABhYWEICgoycuVERK2XSQQIERG1PEYfwiIiopaJAUJERKIwQIiISBQGCBERicIAoXr9+uuv6Nu3L2JjY7XzDh48iIiIiDrrnjt3DhMnTjRkedRK6Tov+/Tpgz179mjnCYKAUaNGwcvLSzvPx8cHN27cMGi9LzoGCNXryJEj+MMf/oCjR4+iurra2OUQAdB9Xr766qs4fPiwdvrcuXPo1KmTMUpsVRggVK8DBw5gwYIF6NOnD06cOGHscogA6D4vnZ2d0bZtW/z0008AgEOHDuGNN94wVpmtBgOE6vjxxx9x9+5dDBo0CBMnTsSBAweMXRJRg+flhAkTcOjQIVRWVuLbb7/FiBEjjFRp68EAoTr279+PwMBASCQSjB49GpcuXUJhYaGxy6JWrqHzMiAgAFlZWUhLS4Ovr6/2Dt/UfIx+O3cyLdXV1Thy5AhkMhlSUlIAAI8fP8bBgwchl8uNXB21Vvqcl+3bt4e7uzsSExOxe/duY5bbajBAqJYTJ06ge/fu2Lt3r3bexYsXERkZiXnz5hmxMmrN9D0vw8LC4Obmhj59+vCJpQbAAKFaDhw4gAReZKoAAAOYSURBVHHjxtWa169fP2g0GiiVSpw6darW2PLEiRMxePBg3Lhxo9b8IUOGYO3atQarm15sDZ2XT/Xs2RM9e/bU2U5ISEitoa3U1FR+W6sReDNFIiIShRfRiYhIFAYIERGJwgAhIiJRGCBERCQKA4SIiERhgBARkSgMECIiEoUBQvSCqampMXYJ1EowQIhEUqlUCA8Px6BBg+Dl5YWYmBjk5eVh5syZ8PLygpeXF9577z3cu3dPu42Pjw927NiBcePGwcPDA++++y4ePXqkXZ6VlYXAwED0798fvr6+OH36NADg/v37WL58OYYNG4bhw4fj448/hlqtBvDkgUrBwcFYs2YNvLy8sGHDBsP+IajVYoAQiaBWqzF37lw4Ojri5MmTOH36NMaMGQNBEDB37lxkZ2cjPT0dBQUFdd7Q09PTsX37dpw4cQLXr1/HwYMHAQCXLl1CZGQkli5digsXLmDPnj1wcnICAERFRcHc3ByZmZk4fPgwcnJysG/fPm2bly5dgrOzM3JycjB//nzD/SGoVeO9sIhEuHTpEoqKirB06VKYmz/5Z+Tp6QkA6Nq1KwDAxsYGISEh+PTTT2ttO2PGDO0dZL29vXHt2jUAT25XPmnSJAwdOhQAIJfLIZfLUVJSglOnTuHChQto27YtLC0t8dZbbyE5ORnBwcEAAHt7e8yYMQMAtPUQNTeeaUQiqFQqODo61nmzLikpQVxcHC5cuIDKykoIgoCOHTvWWsfOzk77ul27digqKtK2OXLkyDr7UiqVqKmpwbBhw7TzNBoNFAqFdtrBwaFJjovoeTBAiERQKBRQqVSoqampFSJJSUmQSCRITU2FtbU1srKyEBMTo3ebeXl5deY7ODhAJpPh7NmzOnsXEolE3IEQNQKvgRCJ0LdvX9jZ2eGvf/0rHjx4gEePHuHbb79FZWUlLC0t0aFDBxQWFmL79u16tzl58mQcPHgQX3/9NTQaDQoLC/Hzzz/D3t4eQ4cOxdq1a1FRUQGNRoO8vDycP3++GY+QqGEMECIRzMzMsHnzZty+fRve3t4YMWIE0tPTER4ejh9++AGenp6YM2cORo8erXebffv2xYcffog1a9bAw8MD06dP1z7rIiEhAY8fP8aYMWMwYMAAREREoLi4uLkOj0gvfB4IERGJwh4IERGJwgAhIiJRGCBERCQKA4SIiERhgBARkSgMECIiEoUBQkREojBAiIhIFAYIERGJ8n9IxFojk8phFwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaMAeEjXWw9d"
      },
      "source": [
        "As one can see, both test and train set are imbalanced. Luckily, both GridSearchCV and BayesSearchCV, which we use for parameter optimization of our models both use stratification by default in classification problems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRTZZ3bqoBN8"
      },
      "source": [
        "## **Data Normalization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9K9jJfDIuS5I"
      },
      "source": [
        "Data rescaling is an important step of the process in developing a good machine learning model. Features often have different units, which often means different scales and distributions of feature variables. These differences, if left unadressed, might result in poor performance of the model, hence the problem should be considered. \n",
        "\n",
        "There are two rescaling techniques used in machine learning - normalization and standardization.\n",
        "Normalization is a rescaling of the data from the original range so that all values are within the range of 0 and 1.\n",
        "Standardizing a dataset involves rescaling the distribution of values so that the mean of observed values is 0 and the standard deviation is 1. This can be thought of as subtracting the mean value or centering the data.\n",
        "Standardization assumes that observations fit a Gaussian distribution with a well behaved mean and standard deviation. [2] In this case, we decided to use normalization, as we didn't want to make a normality assumption about all 7129 genes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FraCLEJgLGvz",
        "outputId": "88455f53-a1a3-46ac-ee00-17cdb6086181",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "source": [
        "# Normalize each feature\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# scale only the numerical features\n",
        "scaler.fit(X_train)\n",
        "X_train= scaler.transform(X_train)\n",
        "X_train = pd.DataFrame(X_train)\n",
        "X_train.columns = gene_names\n",
        "X_train.index = train_ids\n",
        "X_train.head()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>Gene Accession Number</th>\n",
              "      <th>AFFX-BioB-5_at</th>\n",
              "      <th>AFFX-BioB-M_at</th>\n",
              "      <th>AFFX-BioB-3_at</th>\n",
              "      <th>AFFX-BioC-5_at</th>\n",
              "      <th>AFFX-BioC-3_at</th>\n",
              "      <th>AFFX-BioDn-5_at</th>\n",
              "      <th>AFFX-BioDn-3_at</th>\n",
              "      <th>AFFX-CreX-5_at</th>\n",
              "      <th>AFFX-CreX-3_at</th>\n",
              "      <th>AFFX-BioB-5_st</th>\n",
              "      <th>AFFX-BioB-M_st</th>\n",
              "      <th>AFFX-BioB-3_st</th>\n",
              "      <th>AFFX-BioC-5_st</th>\n",
              "      <th>AFFX-BioC-3_st</th>\n",
              "      <th>AFFX-BioDn-5_st</th>\n",
              "      <th>AFFX-BioDn-3_st</th>\n",
              "      <th>AFFX-CreX-5_st</th>\n",
              "      <th>AFFX-CreX-3_st</th>\n",
              "      <th>hum_alu_at</th>\n",
              "      <th>AFFX-DapX-5_at</th>\n",
              "      <th>AFFX-DapX-M_at</th>\n",
              "      <th>AFFX-DapX-3_at</th>\n",
              "      <th>AFFX-LysX-5_at</th>\n",
              "      <th>AFFX-LysX-M_at</th>\n",
              "      <th>AFFX-LysX-3_at</th>\n",
              "      <th>AFFX-PheX-5_at</th>\n",
              "      <th>AFFX-PheX-M_at</th>\n",
              "      <th>AFFX-PheX-3_at</th>\n",
              "      <th>AFFX-ThrX-5_at</th>\n",
              "      <th>AFFX-ThrX-M_at</th>\n",
              "      <th>AFFX-ThrX-3_at</th>\n",
              "      <th>AFFX-TrpnX-5_at</th>\n",
              "      <th>AFFX-TrpnX-M_at</th>\n",
              "      <th>AFFX-TrpnX-3_at</th>\n",
              "      <th>AFFX-HUMISGF3A/M97935_5_at</th>\n",
              "      <th>AFFX-HUMISGF3A/M97935_MA_at</th>\n",
              "      <th>AFFX-HUMISGF3A/M97935_MB_at</th>\n",
              "      <th>AFFX-HUMISGF3A/M97935_3_at</th>\n",
              "      <th>AFFX-HUMRGE/M10098_5_at</th>\n",
              "      <th>AFFX-HUMRGE/M10098_M_at</th>\n",
              "      <th>...</th>\n",
              "      <th>X53065_f_at</th>\n",
              "      <th>X64177_f_at</th>\n",
              "      <th>X67491_f_at</th>\n",
              "      <th>X71345_f_at</th>\n",
              "      <th>X97444_f_at</th>\n",
              "      <th>Z80780_f_at</th>\n",
              "      <th>X00351_f_at</th>\n",
              "      <th>X01677_f_at</th>\n",
              "      <th>M31667_f_at</th>\n",
              "      <th>L41268_f_at</th>\n",
              "      <th>X99479_f_at</th>\n",
              "      <th>HG658-HT658_f_at</th>\n",
              "      <th>M94880_f_at</th>\n",
              "      <th>S80905_f_at</th>\n",
              "      <th>X03068_f_at</th>\n",
              "      <th>Z34822_f_at</th>\n",
              "      <th>U87593_f_at</th>\n",
              "      <th>U88902_cds1_f_at</th>\n",
              "      <th>AC002076_cds2_at</th>\n",
              "      <th>D64015_at</th>\n",
              "      <th>HG2510-HT2606_at</th>\n",
              "      <th>L10717_at</th>\n",
              "      <th>L34355_at</th>\n",
              "      <th>L78833_cds4_at</th>\n",
              "      <th>M13981_at</th>\n",
              "      <th>M21064_at</th>\n",
              "      <th>M93143_at</th>\n",
              "      <th>S78825_at</th>\n",
              "      <th>U11863_at</th>\n",
              "      <th>U29175_at</th>\n",
              "      <th>U48730_at</th>\n",
              "      <th>U58516_at</th>\n",
              "      <th>U73738_at</th>\n",
              "      <th>X06956_at</th>\n",
              "      <th>X16699_at</th>\n",
              "      <th>X83863_at</th>\n",
              "      <th>Z17240_at</th>\n",
              "      <th>L49218_f_at</th>\n",
              "      <th>M71243_f_at</th>\n",
              "      <th>Z78285_f_at</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.531440</td>\n",
              "      <td>0.566775</td>\n",
              "      <td>0.435315</td>\n",
              "      <td>0.289720</td>\n",
              "      <td>0.502041</td>\n",
              "      <td>0.365354</td>\n",
              "      <td>0.755011</td>\n",
              "      <td>0.653759</td>\n",
              "      <td>0.915068</td>\n",
              "      <td>0.542526</td>\n",
              "      <td>0.293706</td>\n",
              "      <td>0.444798</td>\n",
              "      <td>0.347973</td>\n",
              "      <td>0.146568</td>\n",
              "      <td>0.304813</td>\n",
              "      <td>0.908629</td>\n",
              "      <td>0.218638</td>\n",
              "      <td>0.388679</td>\n",
              "      <td>0.220747</td>\n",
              "      <td>0.286638</td>\n",
              "      <td>0.839329</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.406897</td>\n",
              "      <td>0.498408</td>\n",
              "      <td>0.735450</td>\n",
              "      <td>0.624521</td>\n",
              "      <td>0.192118</td>\n",
              "      <td>0.685864</td>\n",
              "      <td>0.289199</td>\n",
              "      <td>0.106952</td>\n",
              "      <td>0.483402</td>\n",
              "      <td>0.310606</td>\n",
              "      <td>0.884932</td>\n",
              "      <td>0.329825</td>\n",
              "      <td>0.393977</td>\n",
              "      <td>0.173093</td>\n",
              "      <td>0.173464</td>\n",
              "      <td>0.167838</td>\n",
              "      <td>0.676343</td>\n",
              "      <td>0.583472</td>\n",
              "      <td>...</td>\n",
              "      <td>0.627957</td>\n",
              "      <td>0.418498</td>\n",
              "      <td>0.228980</td>\n",
              "      <td>0.020283</td>\n",
              "      <td>0.413793</td>\n",
              "      <td>0.540386</td>\n",
              "      <td>0.620751</td>\n",
              "      <td>0.442092</td>\n",
              "      <td>0.223762</td>\n",
              "      <td>0.525903</td>\n",
              "      <td>0.465856</td>\n",
              "      <td>0.853637</td>\n",
              "      <td>0.675489</td>\n",
              "      <td>0.652025</td>\n",
              "      <td>0.250885</td>\n",
              "      <td>0.318538</td>\n",
              "      <td>0.154982</td>\n",
              "      <td>0.307364</td>\n",
              "      <td>0.146965</td>\n",
              "      <td>0.323151</td>\n",
              "      <td>0.073059</td>\n",
              "      <td>0.362745</td>\n",
              "      <td>0.460417</td>\n",
              "      <td>0.466437</td>\n",
              "      <td>0.283677</td>\n",
              "      <td>0.644670</td>\n",
              "      <td>0.397421</td>\n",
              "      <td>0.394261</td>\n",
              "      <td>0.575549</td>\n",
              "      <td>0.392684</td>\n",
              "      <td>0.475460</td>\n",
              "      <td>0.200840</td>\n",
              "      <td>0.133838</td>\n",
              "      <td>0.141710</td>\n",
              "      <td>0.457317</td>\n",
              "      <td>0.282075</td>\n",
              "      <td>0.269663</td>\n",
              "      <td>0.521212</td>\n",
              "      <td>0.060407</td>\n",
              "      <td>0.438462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.683570</td>\n",
              "      <td>0.827362</td>\n",
              "      <td>0.534965</td>\n",
              "      <td>0.745327</td>\n",
              "      <td>0.565306</td>\n",
              "      <td>0.614173</td>\n",
              "      <td>0.165924</td>\n",
              "      <td>0.671982</td>\n",
              "      <td>0.501370</td>\n",
              "      <td>0.372423</td>\n",
              "      <td>0.398601</td>\n",
              "      <td>0.538217</td>\n",
              "      <td>0.814189</td>\n",
              "      <td>0.849722</td>\n",
              "      <td>0.495544</td>\n",
              "      <td>0.541455</td>\n",
              "      <td>0.136201</td>\n",
              "      <td>0.567296</td>\n",
              "      <td>0.116858</td>\n",
              "      <td>0.351293</td>\n",
              "      <td>0.414868</td>\n",
              "      <td>0.315315</td>\n",
              "      <td>0.117241</td>\n",
              "      <td>0.382166</td>\n",
              "      <td>0.346561</td>\n",
              "      <td>0.463602</td>\n",
              "      <td>0.876847</td>\n",
              "      <td>0.863874</td>\n",
              "      <td>0.554007</td>\n",
              "      <td>0.406417</td>\n",
              "      <td>0.558091</td>\n",
              "      <td>0.310606</td>\n",
              "      <td>0.301370</td>\n",
              "      <td>0.463158</td>\n",
              "      <td>0.289837</td>\n",
              "      <td>0.088597</td>\n",
              "      <td>0.119630</td>\n",
              "      <td>0.073120</td>\n",
              "      <td>0.035793</td>\n",
              "      <td>0.032358</td>\n",
              "      <td>...</td>\n",
              "      <td>0.217204</td>\n",
              "      <td>0.650871</td>\n",
              "      <td>0.212880</td>\n",
              "      <td>0.049304</td>\n",
              "      <td>0.557994</td>\n",
              "      <td>0.517559</td>\n",
              "      <td>0.616143</td>\n",
              "      <td>0.494186</td>\n",
              "      <td>0.209068</td>\n",
              "      <td>0.172684</td>\n",
              "      <td>0.097287</td>\n",
              "      <td>0.799018</td>\n",
              "      <td>0.733900</td>\n",
              "      <td>0.295493</td>\n",
              "      <td>0.114361</td>\n",
              "      <td>0.668407</td>\n",
              "      <td>0.077491</td>\n",
              "      <td>0.247599</td>\n",
              "      <td>0.408946</td>\n",
              "      <td>0.266881</td>\n",
              "      <td>0.392694</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.179167</td>\n",
              "      <td>0.234079</td>\n",
              "      <td>0.142631</td>\n",
              "      <td>0.314721</td>\n",
              "      <td>0.218054</td>\n",
              "      <td>0.362380</td>\n",
              "      <td>0.434405</td>\n",
              "      <td>0.118891</td>\n",
              "      <td>0.426380</td>\n",
              "      <td>0.428971</td>\n",
              "      <td>0.358586</td>\n",
              "      <td>0.162987</td>\n",
              "      <td>0.579268</td>\n",
              "      <td>0.276887</td>\n",
              "      <td>0.237828</td>\n",
              "      <td>0.369697</td>\n",
              "      <td>0.024413</td>\n",
              "      <td>0.615385</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.811359</td>\n",
              "      <td>0.905537</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.806075</td>\n",
              "      <td>0.336735</td>\n",
              "      <td>0.220472</td>\n",
              "      <td>0.570156</td>\n",
              "      <td>0.218679</td>\n",
              "      <td>0.789041</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.398601</td>\n",
              "      <td>0.122081</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.573284</td>\n",
              "      <td>0.541889</td>\n",
              "      <td>0.994924</td>\n",
              "      <td>0.942652</td>\n",
              "      <td>0.469182</td>\n",
              "      <td>0.261785</td>\n",
              "      <td>0.665948</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.045045</td>\n",
              "      <td>0.724138</td>\n",
              "      <td>0.345541</td>\n",
              "      <td>0.928571</td>\n",
              "      <td>0.448276</td>\n",
              "      <td>0.403941</td>\n",
              "      <td>0.704188</td>\n",
              "      <td>0.421603</td>\n",
              "      <td>0.240642</td>\n",
              "      <td>0.244813</td>\n",
              "      <td>0.219697</td>\n",
              "      <td>0.509589</td>\n",
              "      <td>0.428070</td>\n",
              "      <td>0.360100</td>\n",
              "      <td>0.221083</td>\n",
              "      <td>0.315389</td>\n",
              "      <td>0.344002</td>\n",
              "      <td>0.268311</td>\n",
              "      <td>0.213161</td>\n",
              "      <td>...</td>\n",
              "      <td>0.619355</td>\n",
              "      <td>0.500999</td>\n",
              "      <td>0.236136</td>\n",
              "      <td>0.033553</td>\n",
              "      <td>0.805643</td>\n",
              "      <td>0.615452</td>\n",
              "      <td>0.714800</td>\n",
              "      <td>0.583798</td>\n",
              "      <td>0.731738</td>\n",
              "      <td>0.474097</td>\n",
              "      <td>0.540692</td>\n",
              "      <td>0.306347</td>\n",
              "      <td>0.298171</td>\n",
              "      <td>0.710782</td>\n",
              "      <td>0.059865</td>\n",
              "      <td>0.493473</td>\n",
              "      <td>0.435424</td>\n",
              "      <td>0.172892</td>\n",
              "      <td>0.178914</td>\n",
              "      <td>0.427653</td>\n",
              "      <td>0.570776</td>\n",
              "      <td>0.477124</td>\n",
              "      <td>0.405208</td>\n",
              "      <td>0.664372</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.555838</td>\n",
              "      <td>0.791325</td>\n",
              "      <td>0.502657</td>\n",
              "      <td>0.589587</td>\n",
              "      <td>0.155759</td>\n",
              "      <td>0.874233</td>\n",
              "      <td>0.682295</td>\n",
              "      <td>0.532828</td>\n",
              "      <td>0.052991</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.444811</td>\n",
              "      <td>0.689139</td>\n",
              "      <td>0.551515</td>\n",
              "      <td>0.071987</td>\n",
              "      <td>0.407692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.691684</td>\n",
              "      <td>0.693811</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.112150</td>\n",
              "      <td>0.248980</td>\n",
              "      <td>0.322835</td>\n",
              "      <td>0.709354</td>\n",
              "      <td>0.478360</td>\n",
              "      <td>0.358904</td>\n",
              "      <td>0.317010</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.335456</td>\n",
              "      <td>0.433559</td>\n",
              "      <td>0.606679</td>\n",
              "      <td>0.424242</td>\n",
              "      <td>0.722504</td>\n",
              "      <td>0.258065</td>\n",
              "      <td>0.972327</td>\n",
              "      <td>0.237972</td>\n",
              "      <td>0.368534</td>\n",
              "      <td>0.736211</td>\n",
              "      <td>0.918919</td>\n",
              "      <td>0.558621</td>\n",
              "      <td>0.585987</td>\n",
              "      <td>0.952381</td>\n",
              "      <td>0.478927</td>\n",
              "      <td>0.172414</td>\n",
              "      <td>0.667539</td>\n",
              "      <td>0.466899</td>\n",
              "      <td>0.272727</td>\n",
              "      <td>0.744813</td>\n",
              "      <td>0.340909</td>\n",
              "      <td>0.591781</td>\n",
              "      <td>0.456140</td>\n",
              "      <td>0.071518</td>\n",
              "      <td>0.117719</td>\n",
              "      <td>0.140837</td>\n",
              "      <td>0.068436</td>\n",
              "      <td>0.230631</td>\n",
              "      <td>0.157093</td>\n",
              "      <td>...</td>\n",
              "      <td>0.468817</td>\n",
              "      <td>0.540394</td>\n",
              "      <td>0.365832</td>\n",
              "      <td>0.020283</td>\n",
              "      <td>0.150470</td>\n",
              "      <td>0.383670</td>\n",
              "      <td>0.509307</td>\n",
              "      <td>0.443938</td>\n",
              "      <td>0.214526</td>\n",
              "      <td>0.618524</td>\n",
              "      <td>0.792329</td>\n",
              "      <td>0.471930</td>\n",
              "      <td>0.590804</td>\n",
              "      <td>0.461495</td>\n",
              "      <td>0.130470</td>\n",
              "      <td>0.234987</td>\n",
              "      <td>0.568266</td>\n",
              "      <td>0.397012</td>\n",
              "      <td>0.252396</td>\n",
              "      <td>0.160772</td>\n",
              "      <td>0.328767</td>\n",
              "      <td>0.081699</td>\n",
              "      <td>0.294792</td>\n",
              "      <td>0.454389</td>\n",
              "      <td>0.412044</td>\n",
              "      <td>0.654822</td>\n",
              "      <td>0.307151</td>\n",
              "      <td>0.379384</td>\n",
              "      <td>0.525778</td>\n",
              "      <td>0.152901</td>\n",
              "      <td>0.644172</td>\n",
              "      <td>0.427572</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.055399</td>\n",
              "      <td>0.012195</td>\n",
              "      <td>0.203774</td>\n",
              "      <td>0.120787</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.040063</td>\n",
              "      <td>0.023077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.750507</td>\n",
              "      <td>0.657980</td>\n",
              "      <td>0.403846</td>\n",
              "      <td>0.476636</td>\n",
              "      <td>0.634694</td>\n",
              "      <td>0.796850</td>\n",
              "      <td>0.537862</td>\n",
              "      <td>0.776765</td>\n",
              "      <td>0.416438</td>\n",
              "      <td>0.601804</td>\n",
              "      <td>0.636364</td>\n",
              "      <td>0.826964</td>\n",
              "      <td>0.531532</td>\n",
              "      <td>0.662338</td>\n",
              "      <td>0.228164</td>\n",
              "      <td>0.624365</td>\n",
              "      <td>0.645161</td>\n",
              "      <td>0.772327</td>\n",
              "      <td>0.298593</td>\n",
              "      <td>0.211207</td>\n",
              "      <td>0.376499</td>\n",
              "      <td>0.351351</td>\n",
              "      <td>0.206897</td>\n",
              "      <td>0.492038</td>\n",
              "      <td>0.415344</td>\n",
              "      <td>0.586207</td>\n",
              "      <td>0.694581</td>\n",
              "      <td>0.693717</td>\n",
              "      <td>0.519164</td>\n",
              "      <td>0.572193</td>\n",
              "      <td>0.769710</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.882192</td>\n",
              "      <td>0.778947</td>\n",
              "      <td>0.335006</td>\n",
              "      <td>0.155865</td>\n",
              "      <td>0.122893</td>\n",
              "      <td>0.086131</td>\n",
              "      <td>0.066572</td>\n",
              "      <td>0.182177</td>\n",
              "      <td>...</td>\n",
              "      <td>0.135484</td>\n",
              "      <td>0.620325</td>\n",
              "      <td>0.324687</td>\n",
              "      <td>0.023951</td>\n",
              "      <td>0.285266</td>\n",
              "      <td>0.460053</td>\n",
              "      <td>0.538969</td>\n",
              "      <td>0.514693</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.095761</td>\n",
              "      <td>0.354537</td>\n",
              "      <td>0.436532</td>\n",
              "      <td>0.522862</td>\n",
              "      <td>0.355961</td>\n",
              "      <td>0.107049</td>\n",
              "      <td>0.963446</td>\n",
              "      <td>0.509225</td>\n",
              "      <td>0.107791</td>\n",
              "      <td>0.421725</td>\n",
              "      <td>0.069132</td>\n",
              "      <td>0.328767</td>\n",
              "      <td>0.153595</td>\n",
              "      <td>0.396875</td>\n",
              "      <td>0.139415</td>\n",
              "      <td>0.299525</td>\n",
              "      <td>0.291878</td>\n",
              "      <td>0.155920</td>\n",
              "      <td>0.526036</td>\n",
              "      <td>0.996172</td>\n",
              "      <td>0.119463</td>\n",
              "      <td>0.386503</td>\n",
              "      <td>0.297411</td>\n",
              "      <td>0.593434</td>\n",
              "      <td>0.187876</td>\n",
              "      <td>0.524390</td>\n",
              "      <td>0.025943</td>\n",
              "      <td>0.255618</td>\n",
              "      <td>0.387879</td>\n",
              "      <td>0.018153</td>\n",
              "      <td>0.530769</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 7129 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "Gene Accession Number  AFFX-BioB-5_at  AFFX-BioB-M_at  ...  M71243_f_at  Z78285_f_at\n",
              "1                            0.531440        0.566775  ...     0.060407     0.438462\n",
              "2                            0.683570        0.827362  ...     0.024413     0.615385\n",
              "3                            0.811359        0.905537  ...     0.071987     0.407692\n",
              "4                            0.691684        0.693811  ...     0.040063     0.023077\n",
              "5                            0.750507        0.657980  ...     0.018153     0.530769\n",
              "\n",
              "[5 rows x 7129 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2kTj0b2y-R2"
      },
      "source": [
        "## **Principal Component Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Imar_w_47AWJ"
      },
      "source": [
        "Since there are 7129 numerical features, a natural step would be to attempt dimensionality reduction, using principal component analysis. Now, note that dimensionality reduction would also make sense from biological point of view, as some of the genes might be linked. We decided to use 30 principal components, as this helps explain around 90 percent of the variance, which is very reasonable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XD645ERWWsRf",
        "outputId": "1726b9f4-2f74-46f2-f79c-313633f4a605",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "n_components = 30\n",
        "pca = PCA(n_components = n_components)\n",
        "X_train_pca = pca.fit_transform(X_train)\n",
        "X_train_pca = pd.DataFrame(X_train_pca)\n",
        "X_train_pca.head()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4.335955</td>\n",
              "      <td>1.873971</td>\n",
              "      <td>-4.492073</td>\n",
              "      <td>0.773770</td>\n",
              "      <td>-2.641525</td>\n",
              "      <td>4.426485</td>\n",
              "      <td>-1.742736</td>\n",
              "      <td>1.060366</td>\n",
              "      <td>-1.985917</td>\n",
              "      <td>2.537553</td>\n",
              "      <td>-2.932625</td>\n",
              "      <td>1.502400</td>\n",
              "      <td>3.065318</td>\n",
              "      <td>-1.588214</td>\n",
              "      <td>-0.305211</td>\n",
              "      <td>5.320206</td>\n",
              "      <td>8.588535</td>\n",
              "      <td>-2.194339</td>\n",
              "      <td>4.668735</td>\n",
              "      <td>0.596885</td>\n",
              "      <td>0.811086</td>\n",
              "      <td>-1.476379</td>\n",
              "      <td>0.281762</td>\n",
              "      <td>-0.586919</td>\n",
              "      <td>-3.808656</td>\n",
              "      <td>-1.267276</td>\n",
              "      <td>1.592440</td>\n",
              "      <td>-2.155034</td>\n",
              "      <td>1.507355</td>\n",
              "      <td>2.538646</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-1.888290</td>\n",
              "      <td>2.243604</td>\n",
              "      <td>2.894183</td>\n",
              "      <td>1.310716</td>\n",
              "      <td>3.339522</td>\n",
              "      <td>-4.195679</td>\n",
              "      <td>0.379701</td>\n",
              "      <td>1.530271</td>\n",
              "      <td>0.615044</td>\n",
              "      <td>2.551174</td>\n",
              "      <td>1.822719</td>\n",
              "      <td>0.612448</td>\n",
              "      <td>5.792718</td>\n",
              "      <td>-0.962809</td>\n",
              "      <td>-4.574907</td>\n",
              "      <td>-2.097889</td>\n",
              "      <td>-3.339450</td>\n",
              "      <td>-0.066150</td>\n",
              "      <td>2.359363</td>\n",
              "      <td>-1.508171</td>\n",
              "      <td>-2.305903</td>\n",
              "      <td>1.370054</td>\n",
              "      <td>-3.205514</td>\n",
              "      <td>-0.413319</td>\n",
              "      <td>-3.610300</td>\n",
              "      <td>-2.110314</td>\n",
              "      <td>5.401324</td>\n",
              "      <td>2.236673</td>\n",
              "      <td>3.129337</td>\n",
              "      <td>-1.052334</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>12.031244</td>\n",
              "      <td>3.846473</td>\n",
              "      <td>-6.740047</td>\n",
              "      <td>9.491940</td>\n",
              "      <td>-0.144719</td>\n",
              "      <td>-1.574215</td>\n",
              "      <td>-5.980327</td>\n",
              "      <td>3.808565</td>\n",
              "      <td>1.018972</td>\n",
              "      <td>0.438956</td>\n",
              "      <td>0.719253</td>\n",
              "      <td>3.145914</td>\n",
              "      <td>0.034067</td>\n",
              "      <td>-4.200813</td>\n",
              "      <td>1.105623</td>\n",
              "      <td>6.347269</td>\n",
              "      <td>-4.480304</td>\n",
              "      <td>2.334223</td>\n",
              "      <td>-4.752565</td>\n",
              "      <td>-2.881092</td>\n",
              "      <td>0.629572</td>\n",
              "      <td>1.449634</td>\n",
              "      <td>1.107507</td>\n",
              "      <td>0.425798</td>\n",
              "      <td>0.150408</td>\n",
              "      <td>0.616999</td>\n",
              "      <td>-0.195955</td>\n",
              "      <td>0.292101</td>\n",
              "      <td>-0.390007</td>\n",
              "      <td>1.498904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3.429125</td>\n",
              "      <td>-1.189050</td>\n",
              "      <td>-5.278071</td>\n",
              "      <td>0.613200</td>\n",
              "      <td>0.322281</td>\n",
              "      <td>3.302466</td>\n",
              "      <td>7.713025</td>\n",
              "      <td>-0.653048</td>\n",
              "      <td>-2.128099</td>\n",
              "      <td>-4.104544</td>\n",
              "      <td>-2.974488</td>\n",
              "      <td>0.883549</td>\n",
              "      <td>-0.526229</td>\n",
              "      <td>-0.768457</td>\n",
              "      <td>-0.413659</td>\n",
              "      <td>0.684794</td>\n",
              "      <td>1.096109</td>\n",
              "      <td>-1.056898</td>\n",
              "      <td>-2.197284</td>\n",
              "      <td>0.952219</td>\n",
              "      <td>0.573854</td>\n",
              "      <td>5.971599</td>\n",
              "      <td>1.199560</td>\n",
              "      <td>-1.409250</td>\n",
              "      <td>5.358893</td>\n",
              "      <td>-3.884200</td>\n",
              "      <td>2.303694</td>\n",
              "      <td>0.739769</td>\n",
              "      <td>4.377079</td>\n",
              "      <td>-2.296540</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-8.867465</td>\n",
              "      <td>7.034777</td>\n",
              "      <td>1.061889</td>\n",
              "      <td>0.976604</td>\n",
              "      <td>0.632451</td>\n",
              "      <td>2.662190</td>\n",
              "      <td>1.001255</td>\n",
              "      <td>-0.258231</td>\n",
              "      <td>-1.865961</td>\n",
              "      <td>-0.411062</td>\n",
              "      <td>-1.586146</td>\n",
              "      <td>4.408844</td>\n",
              "      <td>-0.954805</td>\n",
              "      <td>1.821213</td>\n",
              "      <td>0.516229</td>\n",
              "      <td>-0.851119</td>\n",
              "      <td>-1.169675</td>\n",
              "      <td>-0.152600</td>\n",
              "      <td>-0.351507</td>\n",
              "      <td>-0.635984</td>\n",
              "      <td>1.557372</td>\n",
              "      <td>0.253278</td>\n",
              "      <td>1.274607</td>\n",
              "      <td>-0.047487</td>\n",
              "      <td>0.203232</td>\n",
              "      <td>1.291862</td>\n",
              "      <td>-0.745854</td>\n",
              "      <td>-0.750269</td>\n",
              "      <td>0.448097</td>\n",
              "      <td>3.040338</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          0         1         2   ...        27        28        29\n",
              "0   4.335955  1.873971 -4.492073  ... -2.155034  1.507355  2.538646\n",
              "1  -1.888290  2.243604  2.894183  ...  2.236673  3.129337 -1.052334\n",
              "2  12.031244  3.846473 -6.740047  ...  0.292101 -0.390007  1.498904\n",
              "3   3.429125 -1.189050 -5.278071  ...  0.739769  4.377079 -2.296540\n",
              "4  -8.867465  7.034777  1.061889  ... -0.750269  0.448097  3.040338\n",
              "\n",
              "[5 rows x 30 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txpErDy-XxAb",
        "outputId": "d55fb24e-3e4c-481e-e6a7-2fcb17ea03fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "X_train_pca.reset_index(drop = True, inplace = True)\n",
        "X_train_pca.columns =['PC'+str(i) for i in range(1,n_components+1)]\n",
        "print(\"Percent of explained variance with {n_components} Components : \", round(pca.explained_variance_ratio_.sum()*100,2))\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percent of explained variance with {n_components} Components :  93.02\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eil4CRY--gPs"
      },
      "source": [
        "Once this is done for the train set, we also normalize the test set (separately, as doing the normalization for both sets together could potentially lead to some data leakage). Then we reduce the dimensionality of the test set by projecting the test points into the space defined by the principal components extracted from the train set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VOjhmZcYIKv",
        "outputId": "68741802-2147-48a5-db9f-bc370ef93133",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "# Normalize test set and reduce dimensions through PCA\n",
        "\n",
        "# Normalize\n",
        "\n",
        "X_test = scaler.transform(X_test)\n",
        "X_test = pd.DataFrame(X_test)\n",
        "X_test.columns = gene_names\n",
        "X_test.index = test_ids\n",
        "\n",
        "# PCA Transform\n",
        "X_test_pca = pca.transform(X_test)\n",
        "\n",
        "# generate the matrix with BOTH numerical and categorical features\n",
        "X_test_pca = pd.DataFrame(X_test_pca)\n",
        "X_test_pca.reset_index(drop=True, inplace=True)\n",
        "X_test_pca.columns =['PC'+str(i) for i in range(1,n_components+1)]\n",
        "X_test_pca.head()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PC1</th>\n",
              "      <th>PC2</th>\n",
              "      <th>PC3</th>\n",
              "      <th>PC4</th>\n",
              "      <th>PC5</th>\n",
              "      <th>PC6</th>\n",
              "      <th>PC7</th>\n",
              "      <th>PC8</th>\n",
              "      <th>PC9</th>\n",
              "      <th>PC10</th>\n",
              "      <th>PC11</th>\n",
              "      <th>PC12</th>\n",
              "      <th>PC13</th>\n",
              "      <th>PC14</th>\n",
              "      <th>PC15</th>\n",
              "      <th>PC16</th>\n",
              "      <th>PC17</th>\n",
              "      <th>PC18</th>\n",
              "      <th>PC19</th>\n",
              "      <th>PC20</th>\n",
              "      <th>PC21</th>\n",
              "      <th>PC22</th>\n",
              "      <th>PC23</th>\n",
              "      <th>PC24</th>\n",
              "      <th>PC25</th>\n",
              "      <th>PC26</th>\n",
              "      <th>PC27</th>\n",
              "      <th>PC28</th>\n",
              "      <th>PC29</th>\n",
              "      <th>PC30</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.265514</td>\n",
              "      <td>-3.207727</td>\n",
              "      <td>-2.786487</td>\n",
              "      <td>-1.702472</td>\n",
              "      <td>-2.083942</td>\n",
              "      <td>2.528115</td>\n",
              "      <td>-3.593685</td>\n",
              "      <td>1.640996</td>\n",
              "      <td>-0.429948</td>\n",
              "      <td>1.691263</td>\n",
              "      <td>0.781859</td>\n",
              "      <td>-0.095082</td>\n",
              "      <td>0.681813</td>\n",
              "      <td>-1.551948</td>\n",
              "      <td>-0.249202</td>\n",
              "      <td>-0.458965</td>\n",
              "      <td>1.692169</td>\n",
              "      <td>0.540758</td>\n",
              "      <td>-1.322293</td>\n",
              "      <td>-0.289083</td>\n",
              "      <td>0.049336</td>\n",
              "      <td>-1.589160</td>\n",
              "      <td>-0.111749</td>\n",
              "      <td>1.116507</td>\n",
              "      <td>-0.529986</td>\n",
              "      <td>-0.311122</td>\n",
              "      <td>-0.109920</td>\n",
              "      <td>-0.097864</td>\n",
              "      <td>0.024262</td>\n",
              "      <td>-2.232136</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.311540</td>\n",
              "      <td>-5.013775</td>\n",
              "      <td>-4.847889</td>\n",
              "      <td>-2.141858</td>\n",
              "      <td>0.494876</td>\n",
              "      <td>2.121151</td>\n",
              "      <td>0.521835</td>\n",
              "      <td>-2.747256</td>\n",
              "      <td>4.109137</td>\n",
              "      <td>-1.451260</td>\n",
              "      <td>1.862176</td>\n",
              "      <td>0.827363</td>\n",
              "      <td>-0.913557</td>\n",
              "      <td>0.987720</td>\n",
              "      <td>-0.792933</td>\n",
              "      <td>1.941781</td>\n",
              "      <td>0.312634</td>\n",
              "      <td>-0.825685</td>\n",
              "      <td>1.317022</td>\n",
              "      <td>0.892502</td>\n",
              "      <td>0.138669</td>\n",
              "      <td>0.111230</td>\n",
              "      <td>0.151552</td>\n",
              "      <td>0.074872</td>\n",
              "      <td>-0.264839</td>\n",
              "      <td>-0.868477</td>\n",
              "      <td>-0.133165</td>\n",
              "      <td>0.636512</td>\n",
              "      <td>-0.607513</td>\n",
              "      <td>-1.209986</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-4.169704</td>\n",
              "      <td>-1.028255</td>\n",
              "      <td>3.713443</td>\n",
              "      <td>-3.610079</td>\n",
              "      <td>-2.051088</td>\n",
              "      <td>-3.571384</td>\n",
              "      <td>0.577837</td>\n",
              "      <td>0.177006</td>\n",
              "      <td>-1.160565</td>\n",
              "      <td>2.299942</td>\n",
              "      <td>0.524400</td>\n",
              "      <td>-0.166879</td>\n",
              "      <td>-1.407586</td>\n",
              "      <td>1.618213</td>\n",
              "      <td>-0.290944</td>\n",
              "      <td>1.625727</td>\n",
              "      <td>-0.119102</td>\n",
              "      <td>0.622332</td>\n",
              "      <td>-0.399451</td>\n",
              "      <td>-1.032814</td>\n",
              "      <td>-0.716187</td>\n",
              "      <td>-1.729384</td>\n",
              "      <td>-1.124953</td>\n",
              "      <td>-0.686694</td>\n",
              "      <td>0.176910</td>\n",
              "      <td>-1.892837</td>\n",
              "      <td>-0.847839</td>\n",
              "      <td>1.005687</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>-0.058760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-9.322889</td>\n",
              "      <td>-0.710645</td>\n",
              "      <td>0.340301</td>\n",
              "      <td>-1.982827</td>\n",
              "      <td>-1.071503</td>\n",
              "      <td>1.015475</td>\n",
              "      <td>-1.595842</td>\n",
              "      <td>-1.438380</td>\n",
              "      <td>-0.483306</td>\n",
              "      <td>0.182238</td>\n",
              "      <td>0.224417</td>\n",
              "      <td>-0.485442</td>\n",
              "      <td>-1.352547</td>\n",
              "      <td>0.088184</td>\n",
              "      <td>0.208357</td>\n",
              "      <td>0.871859</td>\n",
              "      <td>-0.322045</td>\n",
              "      <td>-0.048135</td>\n",
              "      <td>-0.293665</td>\n",
              "      <td>-0.319171</td>\n",
              "      <td>0.612974</td>\n",
              "      <td>-0.560726</td>\n",
              "      <td>-0.285509</td>\n",
              "      <td>0.335248</td>\n",
              "      <td>0.300353</td>\n",
              "      <td>0.290180</td>\n",
              "      <td>0.226227</td>\n",
              "      <td>0.191011</td>\n",
              "      <td>-0.507486</td>\n",
              "      <td>-0.626496</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-3.561558</td>\n",
              "      <td>10.602597</td>\n",
              "      <td>-0.151345</td>\n",
              "      <td>-0.831934</td>\n",
              "      <td>-1.669646</td>\n",
              "      <td>2.056914</td>\n",
              "      <td>0.223678</td>\n",
              "      <td>0.659193</td>\n",
              "      <td>-0.212873</td>\n",
              "      <td>-0.645871</td>\n",
              "      <td>-1.112736</td>\n",
              "      <td>0.389284</td>\n",
              "      <td>-0.731572</td>\n",
              "      <td>0.996058</td>\n",
              "      <td>0.525264</td>\n",
              "      <td>2.146968</td>\n",
              "      <td>0.124842</td>\n",
              "      <td>-0.204713</td>\n",
              "      <td>-0.622930</td>\n",
              "      <td>-0.275353</td>\n",
              "      <td>-0.658744</td>\n",
              "      <td>-1.317123</td>\n",
              "      <td>-0.633039</td>\n",
              "      <td>-2.333087</td>\n",
              "      <td>-0.324985</td>\n",
              "      <td>-2.568342</td>\n",
              "      <td>-1.082965</td>\n",
              "      <td>3.582604</td>\n",
              "      <td>-1.942982</td>\n",
              "      <td>-0.442201</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        PC1        PC2       PC3  ...      PC28      PC29      PC30\n",
              "0  0.265514  -3.207727 -2.786487  ... -0.097864  0.024262 -2.232136\n",
              "1  0.311540  -5.013775 -4.847889  ...  0.636512 -0.607513 -1.209986\n",
              "2 -4.169704  -1.028255  3.713443  ...  1.005687  0.001534 -0.058760\n",
              "3 -9.322889  -0.710645  0.340301  ...  0.191011 -0.507486 -0.626496\n",
              "4 -3.561558  10.602597 -0.151345  ...  3.582604 -1.942982 -0.442201\n",
              "\n",
              "[5 rows x 30 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJgroSBKHqBA"
      },
      "source": [
        "Now, before proceeding to the model fitting part, first we need to pick a suitable performance metric."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcTx5h3RVmWS"
      },
      "source": [
        "## **Choosing a Performance Metric**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_plcGAfeN43L"
      },
      "source": [
        "Choosing a suitable performance metric is a crucial part of every machine learning project. In order to select a good metric, we first need to clearly state the purpose of our model.\n",
        "\n",
        "In the case of classifying patients with either ALL or AML diagnosis, the performance of the model in **classifying samples from each of the classes is equally important**. This is, we want a metric which places equal weight on classification performance for each class. We are also **interested in high precision and recall for each of the classes**.  Having these requirements in mind, we decided that a good choice in this case would be to use the **Macro F1-score**. The Macro F1-score is the unweighted average of the F1-score calculated for each of the classes. Using it would enable us to find the model which fits best our requirements. Moreover, this metric works well for our imbalanced dataset, as it would penalize the misclassification of samples from the minority class - in our case AML. This is useful, because it would prevent from obtaining an overly optimistic performance evaluation, in case a model is trained to do well on classifying samples from the majority class only.\n",
        "Having chosen our evaluation metric, we now proceed to model fitting.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3pXOeLkC4Z9"
      },
      "source": [
        "## **Model Fitting**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZUZqN3SC9iv"
      },
      "source": [
        "In this project we decided to compare the performance of XGBoost and SVM Classifiers. We decided also, purely for the sake of exercise, to do parameter optimization using grid search and Bayesian optimization for SVM and XGBoost, respectively. Since we have both a set of numerical and a large set of categorical variables, we decided to do some experimentation with the set of features we used to fit the classifiers. Thus, we would first fit models only with the numerical features and then experiment with adding some of the categorical variables, in order to try and improve the model performance. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uj2CLJPzI1_I"
      },
      "source": [
        "### **Fitting Models Only with the Numerical Features**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MIM-I5kI9Ce"
      },
      "source": [
        "In this section we are going to fit an SVM classifier and an XGBoost classifier, using only the numerical features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhR075ZkKDdj"
      },
      "source": [
        "#### **SVM**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQF12P13KIh1"
      },
      "source": [
        "We decided, purely for the sake of exercise, to tune the hyperparameters of the SVM classifier trained, using GridSearchCV.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOBkwDO3Oe8Q"
      },
      "source": [
        "##### **Grid Search Hyperparameter Tuning**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QGjK8RzY_Wf",
        "outputId": "ab5e28e5-e873-4338-d84f-12afbd1986c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Set the parameters by cross-validation\n",
        "tuned_parameters = [{'kernel': ['linear'], 'C': [1e-1, 1, 10, 100, 1000]},\n",
        "                    {'kernel': ['poly'], 'C': [1e-1, 1, 10, 100, 1000]},\n",
        "                    {'kernel': ['sigmoid'], 'C': [1e-1, 1, 10, 100, 1000]},\n",
        "                    {'kernel': ['rbf'], 'gamma': [1e-4, 1e-2, 1, 5, 10],\n",
        "                     'C': [1e-1, 1, 10, 100, 1000]},]\n",
        "\n",
        "scoring = {'f1_macro': make_scorer(f1_score, average='macro')}\n",
        "clf = GridSearchCV(svm.SVC(), tuned_parameters, scoring = scoring, refit = 'f1_macro')\n",
        "clf.fit(X_train_pca, y_train)\n",
        "print(\"Best parameters set through GridSearchCV\")\n",
        "print()\n",
        "print(clf.best_params_)\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best parameters set through GridSearchCV\n",
            "\n",
            "{'C': 0.1, 'kernel': 'linear'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWFEEXExKyCT"
      },
      "source": [
        "As mentioned earlier, our dataset is imbalanced (there are more ALL than AML samples in both the train and test set. Hence it is very important to ensure that the splitof the train sample during the cross-validation used is stratified. Luckily, GridSearchCV does this by default. In addition, we used a 5-fold cross-validation, as our train set is not very big and we wanted to ensure that we still had several samples to tune our hyperparameters on. We used the macro f1-score, in order to evaluate our models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msJavbdVKYQD"
      },
      "source": [
        "We then use the best hyperparameter values and instantiate the best model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3D_FVhWOmPr"
      },
      "source": [
        "##### **Best Model Fitting**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPCz_EJVZAE4",
        "outputId": "05e0d93e-489f-4f35-a9bf-8e205b07fc24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Instantiating Best Model\n",
        "clf = svm.SVC(kernel='linear', C = 0.1)\n",
        "clf.fit(X_train_pca, y_train)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=0.1, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
              "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
              "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
              "    tol=0.001, verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dm8f4KI7KjCd"
      },
      "source": [
        "We then use the trained model to classify samples in the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsDlSAIXZDTq"
      },
      "source": [
        "# Generate predictions for test samples\n",
        "y_pred = clf.predict(X_test_pca)\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4S9TFo8EOt-N"
      },
      "source": [
        "##### **Model Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WG9L4VQiZFKO",
        "outputId": "6ed2f9e6-a209-4d84-b8aa-b5b59e27ddb1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Calculate model performance metrics on test set\n",
        "\n",
        "print(\"Macro F1-Score\")\n",
        "print(f1_score(y_test, y_pred, average = \"macro\"))\n",
        "print()\n",
        "print(\"F1-score for each label:\")\n",
        "print()\n",
        "print(\" ALL       AML\")\n",
        "\n",
        "print(f1_score(y_test, y_pred, average = None))\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Macro F1-Score\n",
            "0.7984189723320159\n",
            "\n",
            "F1-score for each label:\n",
            "\n",
            " ALL       AML\n",
            "[0.86956522 0.72727273]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiGiRWe8PQ44"
      },
      "source": [
        "This is a reasonable performance result, although there is clearly room for improvement. As it can be expected, the F1-score for AML is lower, as we had almost 2:1 ratio of ALL to AML samples in the train set. In order to get a better idea of the performance of the trained classifier, it is worth investigating how well the model does, compared to a one-outcome-only prediction for all samples (only ALL or only AML.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAi54mNYO1X1"
      },
      "source": [
        "##### **Model Performance Comparison to One-Outcome-Only Predictions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XG1gPOqwQY5A"
      },
      "source": [
        "# generate vectors will all 1s (i.e AML) or all 0s (i.e ALL)\n",
        "aml = [1] * len(y_pred)\n",
        "all = [0] * len(y_pred)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBoEGJz2QtyG",
        "outputId": "1386af0d-6046-47cc-a7c6-667c58736160",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# macro score of only-ALL macro F1-score\n",
        "print(\"Macro F1-Score of only-ALL predictions\")\n",
        "print(f1_score(y_test, all, average = \"macro\"))\n",
        "print()\n",
        "print(\"F1-score for each label\")\n",
        "print()\n",
        "print(\" ALL       AML\")\n",
        "print(f1_score(y_test, all, average = None))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Macro F1-Score of only-ALL predictions\n",
            "0.37037037037037035\n",
            "\n",
            "F1-score for each label\n",
            "\n",
            " ALL       AML\n",
            "[0.74074074 0.        ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCaL_EtnRQUD"
      },
      "source": [
        "Now, it is worth pointing out, that when considering an only-ALL-predictions set, the F1-score for AML samples predictions is not defined, as both Precision and Recall are 0, which means that AML F1-score is 0/0. In this case, however, sklearn assigns AML F1-score = 0, thus still meaning in has no contribution to the macro F1 score, thus bringing its value down. Hence, we decided to stick to macro F1-score as an evaluation metric.\n",
        "\n",
        "As one can see, our SVM classifier does significantly better in classifying samples, compared to an only-ALL-predictions dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsGlzRasreuk"
      },
      "source": [
        "We also need to check how it does against an only-AML-predictions dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QonRZ-NMQY5H",
        "outputId": "1487f9c5-fab4-43e7-dbaf-b40fd7118562",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"Macro F1-Score of only-ALL predictions\")\n",
        "print(f1_score(y_test, aml, average = \"macro\"))\n",
        "print()\n",
        "print(\"F1-score for each label\")\n",
        "print()\n",
        "print(\" ALL       AML\")\n",
        "print(f1_score(y_test, aml, average = None))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Macro F1-Score of only-ALL predictions\n",
            "0.2916666666666667\n",
            "\n",
            "F1-score for each label\n",
            "\n",
            " ALL       AML\n",
            "[0.         0.58333333]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIFmCMaWRqyM"
      },
      "source": [
        "Unsurprisingly, our model does better, compared to an only-AML-predictions dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnfcWgZrsNtK"
      },
      "source": [
        "#### **XGBoost Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jv3cDpMGa6yg",
        "outputId": "54a3a523-e9c0-4715-8cd2-0e20f2e838ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "pip install scikit-optimize"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-optimize in /usr/local/lib/python3.6/dist-packages (0.8.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (0.17.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (1.18.5)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (1.4.1)\n",
            "Requirement already satisfied: pyaml>=16.9 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (20.4.0)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (0.22.2.post1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from pyaml>=16.9->scikit-optimize) (3.13)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-ZanqMosh-I"
      },
      "source": [
        "##### **Bayesian Optimization Hyperparameter Tuning**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n65n12ygbAOo",
        "outputId": "3261a904-b355-4ad6-f47c-29787d3703a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import skopt \n",
        "from skopt import BayesSearchCV\n",
        "import xgboost as xgb\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', message='The objective has been evaluated at this point before.')\n",
        "\n",
        "parameters = {'n_estimators': skopt.space.Integer(0,100),\n",
        "        'min_num_child': skopt.space.Integer(0, 50,),\n",
        "        'max_depth': skopt.space.Integer(0, 10),\n",
        "        'subsample': skopt.space.Real(0.5, 1.0),\n",
        "        'colsample_bytree': skopt.space.Real(0.5, 1.0),\n",
        "        'reg_lambda': skopt.space.Real(1e-10,100,'log-uniform'),\n",
        "        'reg_alpha': skopt.space.Real(1e-10,100,'log-uniform'),\n",
        "        'learning-rate': skopt.space.Real(0.01,0.2,'log-uniform'),\n",
        "        }\n",
        "\n",
        "bayes = BayesSearchCV(xgb.XGBClassifier(), search_spaces= parameters, n_iter=10, scoring='f1_macro',cv=5,random_state=0)\n",
        "res = bayes.fit(X_train_pca, y_train)\n",
        "print(res.best_params_)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OrderedDict([('colsample_bytree', 0.7643458710377254), ('learning-rate', 0.011893969796789843), ('max_depth', 3), ('min_num_child', 10), ('n_estimators', 24), ('reg_alpha', 1.2750414511557992e-06), ('reg_lambda', 0.005068685293864641), ('subsample', 0.7042865059246077)])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQoUIp7usrkV"
      },
      "source": [
        "##### **Best Model Fitting**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQo7tu0ubE3K"
      },
      "source": [
        "final_params={'objective': 'binary:logistic', 'n_estimators': 24, 'colsample_bytree': 0.7643458710377254, 'learning-rate': 0.011893969796789843,'max_depth': 3, 'min_num_child': 10, 'reg_alpha': 1.2750414511557992e-06, 'reg_lambda': 0.005068685293864641 , 'subsample': 0.7042865059246077,}\n",
        "dtrain = xgb.DMatrix(X_train_pca, y_train)\n",
        "model_xgb = xgb.train(final_params,dtrain=dtrain)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIVHj_BYbHhQ"
      },
      "source": [
        "dtest = xgb.DMatrix(X_test_pca, y_test)\n",
        "y_pred = model_xgb.predict(dtest)\n",
        "# turn the probabilities into labels\n",
        "y_pred[y_pred >= 0.5] = 1\n",
        "y_pred[y_pred < 0.5] = 0"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0SIi0_osv7d"
      },
      "source": [
        "##### **Model Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dn8qV9kMbL2q",
        "outputId": "f98d5b1e-35d6-4051-a1ee-726be20f857e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"Macro F1-Score\")\n",
        "print(f1_score(y_test, y_pred, average = \"macro\"))\n",
        "print()\n",
        "print(\"F1-score for each label\")\n",
        "print()\n",
        "print(\" ALL       AML\")\n",
        "print(f1_score(y_test, y_pred, average = None))\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Macro F1-Score\n",
            "0.8068181818181818\n",
            "\n",
            "F1-score for each label\n",
            "\n",
            " ALL       AML\n",
            "[0.86363636 0.75      ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyzBiZims3ZT"
      },
      "source": [
        "As one can see, the XGBoost Classifier performs almost as well in classifying the samples from the two classes, compared to the SVM Classifier. \n",
        "\n",
        "It also does better than the only-ALL-predictions and only-AML-predictions, which have macro F1-scores of 0.37 and 0.29, respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-S08yCfZuGDr"
      },
      "source": [
        "We will now try and imporve the performance of both classifiers by adding some extra categorical variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYFiUmzoyZV_"
      },
      "source": [
        "### **Fitting Models with Both Numerical and Categorical Features**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OICfPvHysiE"
      },
      "source": [
        "There are 7129 categorical variables, whose values denote whether the signal from a gene is strong enough for it to be qualified as expressed (\"P\" - present), absent (\"A\"), or marginal (\"M\"). In order for the classifier to be able to process these categorical features, they were encoded with numbers - 1, 0, and 2, respectively. Now, since this is a very large number of features (considering that we only have 38 train samples), we needed to carefully select a small subset of the categorical variables to add to the set of numerical features.\n",
        "\n",
        "The idea we tried out was to find differentially expressed genes between the two groups (ALL and AML) from the provided gene expression microarray data of the train set.\n",
        "The categorical variables, corresponding to these  genes will be added to the numerical features. Since there is a well-established package for microarray expression data in R - limma, it was used. The data was already normalized, so this step was skipped."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ij8955XD8nIM"
      },
      "source": [
        "#### **Categorical Variables Selection**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "460FzroP75Xy"
      },
      "source": [
        "We selected the 10 differentially expressed genes in the train set with the smallest p-value, after Benjamini-Hochberg adjustment. These were (in order of smallest to largest adj. p-value): *U50136_rna1_at, X95735_at, M55150_at, M16038_at, Y12670_at, M23197_at, X17042_at, U82759_at, D49950_at, M84526_at*. The categorical variables, corresponding to these variables were extracted and added to the numerical variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xGpazz65fSj"
      },
      "source": [
        ""
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdQ7lwnecKj7"
      },
      "source": [
        "# extract the categorical variables, which correspond to the top 10 differentially expressed genes between the two classes\n",
        "X_train_de = X_train_call[[\"Call U50136_rna1_at\", \"Call X95735_at\", \"Call M55150_at\", \"Call M16038_at\", \"Call Y12670_at\", \"Call M23197_at\", \"Call X17042_at\", \"Call U82759_at\", \"Call D49950_at\", \"Call M84526_at\"]]\n",
        "X_test_de = X_test_call[[\"Call U50136_rna1_at\", \"Call X95735_at\", \"Call M55150_at\", \"Call M16038_at\", \"Call Y12670_at\", \"Call M23197_at\", \"Call X17042_at\", \"Call U82759_at\", \"Call D49950_at\", \"Call M84526_at\"]]\n",
        "X_train_de.reset_index(drop = True, inplace = True)\n",
        "X_test_de.reset_index(drop = True, inplace = True)\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqEKeMdqcXiF"
      },
      "source": [
        "# add the \"de\" categorical variables to the numerical variables \n",
        "X_train_all = pd.concat([X_train_pca, X_train_de], axis=1, sort=False)\n",
        "X_test_all = pd.concat([X_test_pca, X_test_de], axis=1, sort=False)\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjoEE-jRdOK_",
        "outputId": "059ff031-aee6-46f4-ea34-5fb2b9c3c0ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "X_train_all.head()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PC1</th>\n",
              "      <th>PC2</th>\n",
              "      <th>PC3</th>\n",
              "      <th>PC4</th>\n",
              "      <th>PC5</th>\n",
              "      <th>PC6</th>\n",
              "      <th>PC7</th>\n",
              "      <th>PC8</th>\n",
              "      <th>PC9</th>\n",
              "      <th>PC10</th>\n",
              "      <th>PC11</th>\n",
              "      <th>PC12</th>\n",
              "      <th>PC13</th>\n",
              "      <th>PC14</th>\n",
              "      <th>PC15</th>\n",
              "      <th>PC16</th>\n",
              "      <th>PC17</th>\n",
              "      <th>PC18</th>\n",
              "      <th>PC19</th>\n",
              "      <th>PC20</th>\n",
              "      <th>PC21</th>\n",
              "      <th>PC22</th>\n",
              "      <th>PC23</th>\n",
              "      <th>PC24</th>\n",
              "      <th>PC25</th>\n",
              "      <th>PC26</th>\n",
              "      <th>PC27</th>\n",
              "      <th>PC28</th>\n",
              "      <th>PC29</th>\n",
              "      <th>PC30</th>\n",
              "      <th>Call U50136_rna1_at</th>\n",
              "      <th>Call X95735_at</th>\n",
              "      <th>Call M55150_at</th>\n",
              "      <th>Call M16038_at</th>\n",
              "      <th>Call Y12670_at</th>\n",
              "      <th>Call M23197_at</th>\n",
              "      <th>Call X17042_at</th>\n",
              "      <th>Call U82759_at</th>\n",
              "      <th>Call D49950_at</th>\n",
              "      <th>Call M84526_at</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4.335955</td>\n",
              "      <td>1.873971</td>\n",
              "      <td>-4.492073</td>\n",
              "      <td>0.773770</td>\n",
              "      <td>-2.641525</td>\n",
              "      <td>4.426485</td>\n",
              "      <td>-1.742736</td>\n",
              "      <td>1.060366</td>\n",
              "      <td>-1.985917</td>\n",
              "      <td>2.537553</td>\n",
              "      <td>-2.932625</td>\n",
              "      <td>1.502400</td>\n",
              "      <td>3.065318</td>\n",
              "      <td>-1.588214</td>\n",
              "      <td>-0.305211</td>\n",
              "      <td>5.320206</td>\n",
              "      <td>8.588535</td>\n",
              "      <td>-2.194339</td>\n",
              "      <td>4.668735</td>\n",
              "      <td>0.596885</td>\n",
              "      <td>0.811086</td>\n",
              "      <td>-1.476379</td>\n",
              "      <td>0.281762</td>\n",
              "      <td>-0.586919</td>\n",
              "      <td>-3.808656</td>\n",
              "      <td>-1.267276</td>\n",
              "      <td>1.592440</td>\n",
              "      <td>-2.155034</td>\n",
              "      <td>1.507355</td>\n",
              "      <td>2.538646</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-1.888290</td>\n",
              "      <td>2.243604</td>\n",
              "      <td>2.894183</td>\n",
              "      <td>1.310716</td>\n",
              "      <td>3.339522</td>\n",
              "      <td>-4.195679</td>\n",
              "      <td>0.379701</td>\n",
              "      <td>1.530271</td>\n",
              "      <td>0.615044</td>\n",
              "      <td>2.551174</td>\n",
              "      <td>1.822719</td>\n",
              "      <td>0.612448</td>\n",
              "      <td>5.792718</td>\n",
              "      <td>-0.962809</td>\n",
              "      <td>-4.574907</td>\n",
              "      <td>-2.097889</td>\n",
              "      <td>-3.339450</td>\n",
              "      <td>-0.066150</td>\n",
              "      <td>2.359363</td>\n",
              "      <td>-1.508171</td>\n",
              "      <td>-2.305903</td>\n",
              "      <td>1.370054</td>\n",
              "      <td>-3.205514</td>\n",
              "      <td>-0.413319</td>\n",
              "      <td>-3.610300</td>\n",
              "      <td>-2.110314</td>\n",
              "      <td>5.401324</td>\n",
              "      <td>2.236673</td>\n",
              "      <td>3.129337</td>\n",
              "      <td>-1.052334</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>12.031244</td>\n",
              "      <td>3.846473</td>\n",
              "      <td>-6.740047</td>\n",
              "      <td>9.491940</td>\n",
              "      <td>-0.144719</td>\n",
              "      <td>-1.574215</td>\n",
              "      <td>-5.980327</td>\n",
              "      <td>3.808565</td>\n",
              "      <td>1.018972</td>\n",
              "      <td>0.438956</td>\n",
              "      <td>0.719253</td>\n",
              "      <td>3.145914</td>\n",
              "      <td>0.034067</td>\n",
              "      <td>-4.200813</td>\n",
              "      <td>1.105623</td>\n",
              "      <td>6.347269</td>\n",
              "      <td>-4.480304</td>\n",
              "      <td>2.334223</td>\n",
              "      <td>-4.752565</td>\n",
              "      <td>-2.881092</td>\n",
              "      <td>0.629572</td>\n",
              "      <td>1.449634</td>\n",
              "      <td>1.107507</td>\n",
              "      <td>0.425798</td>\n",
              "      <td>0.150408</td>\n",
              "      <td>0.616999</td>\n",
              "      <td>-0.195955</td>\n",
              "      <td>0.292101</td>\n",
              "      <td>-0.390007</td>\n",
              "      <td>1.498904</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3.429125</td>\n",
              "      <td>-1.189050</td>\n",
              "      <td>-5.278071</td>\n",
              "      <td>0.613200</td>\n",
              "      <td>0.322281</td>\n",
              "      <td>3.302466</td>\n",
              "      <td>7.713025</td>\n",
              "      <td>-0.653048</td>\n",
              "      <td>-2.128099</td>\n",
              "      <td>-4.104544</td>\n",
              "      <td>-2.974488</td>\n",
              "      <td>0.883549</td>\n",
              "      <td>-0.526229</td>\n",
              "      <td>-0.768457</td>\n",
              "      <td>-0.413659</td>\n",
              "      <td>0.684794</td>\n",
              "      <td>1.096109</td>\n",
              "      <td>-1.056898</td>\n",
              "      <td>-2.197284</td>\n",
              "      <td>0.952219</td>\n",
              "      <td>0.573854</td>\n",
              "      <td>5.971599</td>\n",
              "      <td>1.199560</td>\n",
              "      <td>-1.409250</td>\n",
              "      <td>5.358893</td>\n",
              "      <td>-3.884200</td>\n",
              "      <td>2.303694</td>\n",
              "      <td>0.739769</td>\n",
              "      <td>4.377079</td>\n",
              "      <td>-2.296540</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-8.867465</td>\n",
              "      <td>7.034777</td>\n",
              "      <td>1.061889</td>\n",
              "      <td>0.976604</td>\n",
              "      <td>0.632451</td>\n",
              "      <td>2.662190</td>\n",
              "      <td>1.001255</td>\n",
              "      <td>-0.258231</td>\n",
              "      <td>-1.865961</td>\n",
              "      <td>-0.411062</td>\n",
              "      <td>-1.586146</td>\n",
              "      <td>4.408844</td>\n",
              "      <td>-0.954805</td>\n",
              "      <td>1.821213</td>\n",
              "      <td>0.516229</td>\n",
              "      <td>-0.851119</td>\n",
              "      <td>-1.169675</td>\n",
              "      <td>-0.152600</td>\n",
              "      <td>-0.351507</td>\n",
              "      <td>-0.635984</td>\n",
              "      <td>1.557372</td>\n",
              "      <td>0.253278</td>\n",
              "      <td>1.274607</td>\n",
              "      <td>-0.047487</td>\n",
              "      <td>0.203232</td>\n",
              "      <td>1.291862</td>\n",
              "      <td>-0.745854</td>\n",
              "      <td>-0.750269</td>\n",
              "      <td>0.448097</td>\n",
              "      <td>3.040338</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         PC1       PC2  ...  Call D49950_at  Call M84526_at\n",
              "0   4.335955  1.873971  ...               0               0\n",
              "1  -1.888290  2.243604  ...               0               0\n",
              "2  12.031244  3.846473  ...               0               0\n",
              "3   3.429125 -1.189050  ...               0               0\n",
              "4  -8.867465  7.034777  ...               0               0\n",
              "\n",
              "[5 rows x 40 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QX0ibth8UTi"
      },
      "source": [
        "Now that we have selected and added the categorical variables to the set of numeric features, we proceed with training an SVM and XGBoost classifiers once again, in order to try and improve the results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBRY3uas8gBR"
      },
      "source": [
        "#### **SVM**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLh6FkBx9uT9"
      },
      "source": [
        "##### **Grid Search Hyperparameter Tuning**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ju_YYrVcdIt5",
        "outputId": "c011cbcd-7bfa-4bac-a800-a29154aeae61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Set the parameters by cross-validation\n",
        "tuned_parameters = [{'kernel': ['linear'], 'C': [1e-1, 1, 10, 100, 1000]},\n",
        "                    {'kernel': ['poly'], 'C': [1e-1, 1, 10, 100, 1000]},\n",
        "                    {'kernel': ['sigmoid'], 'C': [1e-1, 1, 10, 100, 1000]},\n",
        "                    {'kernel': ['rbf'], 'gamma': [1e-4, 1e-2, 1, 5, 10],\n",
        "                     'C': [1e-1, 1, 10, 100, 1000]},]\n",
        "\n",
        "scoring = {'f1_macro': make_scorer(f1_score, average='macro')}\n",
        "clf = GridSearchCV(svm.SVC(), tuned_parameters, scoring = scoring, refit = 'f1_macro')\n",
        "clf.fit(X_train_all, y_train)\n",
        "print(\"Best parameters set through GridSearchCV\")\n",
        "print()\n",
        "print(clf.best_params_)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best parameters set through GridSearchCV\n",
            "\n",
            "{'C': 0.1, 'kernel': 'linear'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrkYSkOq91xM"
      },
      "source": [
        "##### **Best Model Fitting**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PxWKhjGeB2t",
        "outputId": "7edb88c4-d9dc-4f8a-9a36-5a15ad614299",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Instantiating Best Model\n",
        "clf = svm.SVC(kernel='linear', C = 0.1)\n",
        "clf.fit(X_train_all, y_train)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=0.1, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
              "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
              "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
              "    tol=0.001, verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQ0b48xr9_Qo"
      },
      "source": [
        "##### **Model Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBvBycfneN1n"
      },
      "source": [
        "# Generate predictions for test samples\n",
        "y_pred = clf.predict(X_test_all)\n"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjBq7OLcePo8",
        "outputId": "be798dc0-f615-47fc-acb4-a5260b20f470",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Calculate model performance metrics on test set\n",
        "\n",
        "print(\"Macro F1-Score\")\n",
        "print(f1_score(y_test, y_pred, average = \"macro\"))\n",
        "print()\n",
        "print(\"F1-score for each label\")\n",
        "print()\n",
        "print(\" ALL       AML\")\n",
        "print(f1_score(y_test, y_pred, average = None))\n",
        "\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Macro F1-Score\n",
            "0.7984189723320159\n",
            "\n",
            "F1-score for each label\n",
            "\n",
            " ALL       AML\n",
            "[0.86956522 0.72727273]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVTY2Trt-XtA"
      },
      "source": [
        "As one can see, adding the categorical variables does not change the performance of the SVM classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0Tp5BhR_0Qt"
      },
      "source": [
        "#### **XGBoost Classifier**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQZZCmcQ_6l1"
      },
      "source": [
        "##### **Bayesian Optimization Hyperparameter Tuning**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bs75Zusegzb",
        "outputId": "98f3af45-7165-4224-aeb3-9122cbf17b46",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import skopt \n",
        "from skopt import BayesSearchCV\n",
        "import xgboost as xgb\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', message='The objective has been evaluated at this point before.')\n",
        "\n",
        "parameters = {'n_estimators': skopt.space.Integer(0,100),\n",
        "        'min_num_child': skopt.space.Integer(0, 50,),\n",
        "        'max_depth': skopt.space.Integer(0, 10),\n",
        "        'subsample': skopt.space.Real(0.5, 1.0),\n",
        "        'colsample_bytree': skopt.space.Real(0.5, 1.0),\n",
        "        'reg_lambda': skopt.space.Real(1e-10,100,'log-uniform'),\n",
        "        'reg_alpha': skopt.space.Real(1e-10,100,'log-uniform'),\n",
        "        'learning-rate': skopt.space.Real(0.01,0.2,'log-uniform'),\n",
        "        }\n",
        "\n",
        "bayes = BayesSearchCV(xgb.XGBClassifier(objective = 'binary:logistic'), search_spaces= parameters, n_iter=10, scoring='f1_macro',cv=5,random_state=0)\n",
        "res = bayes.fit(X_train_all,y_train)\n",
        "print(res.best_params_)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OrderedDict([('colsample_bytree', 0.6162570141151325), ('learning-rate', 0.020729429554412183), ('max_depth', 7), ('min_num_child', 14), ('n_estimators', 18), ('reg_alpha', 1.1046982314481511e-07), ('reg_lambda', 1.551926920387429e-10), ('subsample', 0.5148603857041089)])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNgbWJ6tAB7k"
      },
      "source": [
        "##### **Best Model Fitting**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JITprn8ep2F"
      },
      "source": [
        "final_params={'objective': 'binary:logistic', 'n_estimators': 18, 'colsample_bytree': 0.6162570141151325, 'learning-rate': 0.020729429554412183,'max_depth': 7, 'min_num_child': 14, 'reg_alpha': 1.1046982314481511e-07, 'reg_lambda': 1.551926920387429e-10 , 'subsample': 0.5148603857041089}\n",
        "dtrain = xgb.DMatrix(X_train_all, y_train)\n",
        "model_xgb_pca_num_cat_de = xgb.train(final_params, dtrain = dtrain)\n"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJa3gLzJAGEB"
      },
      "source": [
        "##### **Model Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8nlDZepessP"
      },
      "source": [
        "dtest = xgb.DMatrix(X_test_all, y_test)\n",
        "y_pred = model_xgb_pca_num_cat_de.predict(dtest)\n",
        "# turn the probabilities into labels\n",
        "y_pred[y_pred >= 0.5] = 1\n",
        "y_pred[y_pred < 0.5] = 0"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6T4ajh4evDB",
        "outputId": "ead94323-6277-4a0e-9b26-58f8302be562",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"Macro F1-Score\")\n",
        "print(f1_score(y_test, y_pred, average = \"macro\"))\n",
        "print()\n",
        "print(\"F1-score for each label\")\n",
        "print()\n",
        "print(\" ALL       AML\")\n",
        "print(f1_score(y_test, y_pred, average = None))\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Macro F1-Score\n",
            "0.8418604651162791\n",
            "\n",
            "F1-score for each label\n",
            "\n",
            " ALL       AML\n",
            "[0.88372093 0.8       ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-lOq6i0BDU_"
      },
      "source": [
        "It seems like adding the categorical features resulted in a slight improvement of the performance of the XGBoost classifier, making this also an overall well-performing classification model, at least based on our test data. As one can see, this model does reasonably well in classifying samples from both the ALL and AML classes, which is exactly what we set as a goal at the beginning of the project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pF0up5gGD6AD"
      },
      "source": [
        "Now, one cannot help but ask how well a model with a reduced number of features would do. Thus, we are now going to train a classifier with only the categorical variables, corresponding to the top 10 differentially expressed genes, as well as a model with said categorical variables, as well as the numerical variables, corresponding to these 10 genes. We will also experiment with a classifier trained on both the numerical and categorical features that correspond to the DE genes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asRMrV_MJZ5G"
      },
      "source": [
        "### **Fitting a Model with Numerical and Categorical Features Corresponding to DE Genes**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qys0NMUjJ3Kc"
      },
      "source": [
        "Taking differentially expressed genes into account seems to add some extra information and improves the performance of the XGBoost classfier. Thus we are now going to train an XGBoost classifier, which uses the DE categorical variables, as well as the numerical variables, corresponding to the 10 differentially expressed genes mentioned above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6Q8SyRILVOt"
      },
      "source": [
        "# extract the numerical variables, which correspond to the top 10 differentially expressed genes between the two classes\n",
        "X_train_num_de = X_train[[\"U50136_rna1_at\", \"X95735_at\", \"M55150_at\", \"M16038_at\", \"Y12670_at\", \"M23197_at\", \"X17042_at\", \"U82759_at\", \"D49950_at\", \"M84526_at\"]]\n",
        "X_test_num_de = X_test[[\"U50136_rna1_at\", \"X95735_at\", \"M55150_at\", \"M16038_at\", \"Y12670_at\", \"M23197_at\", \"X17042_at\", \"U82759_at\", \"D49950_at\", \"M84526_at\"]]\n",
        "X_train_num_de.reset_index(drop = True, inplace = True)\n",
        "X_test_num_de.reset_index(drop = True, inplace = True)\n",
        "\n"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJ5N2YOOLVO5"
      },
      "source": [
        "# add the \"de\" categorical variables to the numerical variables \n",
        "X_train_all_de = pd.concat([X_train_num_de, X_train_de], axis=1, sort=False)\n",
        "X_test_all_de = pd.concat([X_test_num_de, X_test_de], axis=1, sort=False)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tT96jTZ2Pe37",
        "outputId": "d9952c11-2956-4137-a1ae-51898a7d3bd7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import skopt \n",
        "from skopt import BayesSearchCV\n",
        "import xgboost as xgb\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', message='The objective has been evaluated at this point before.')\n",
        "\n",
        "parameters = {'n_estimators': skopt.space.Integer(0,100),\n",
        "        'min_num_child': skopt.space.Integer(0, 50,),\n",
        "        'max_depth': skopt.space.Integer(0, 10),\n",
        "        'subsample': skopt.space.Real(0.5, 1.0),\n",
        "        'colsample_bytree': skopt.space.Real(0.5, 1.0),\n",
        "        'reg_lambda': skopt.space.Real(1e-10,100,'log-uniform'),\n",
        "        'reg_alpha': skopt.space.Real(1e-10,100,'log-uniform'),\n",
        "        'learning-rate': skopt.space.Real(0.01,0.2,'log-uniform'),\n",
        "        }\n",
        "\n",
        "bayes = BayesSearchCV(xgb.XGBClassifier(objective = 'binary:logistic'), search_spaces= parameters, n_iter=10, scoring='f1_macro',cv=5,random_state=0)\n",
        "res = bayes.fit(X_train_all_de,y_train)\n",
        "print(res.best_params_)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OrderedDict([('colsample_bytree', 0.6162570141151325), ('learning-rate', 0.020729429554412183), ('max_depth', 7), ('min_num_child', 14), ('n_estimators', 18), ('reg_alpha', 1.1046982314481511e-07), ('reg_lambda', 1.551926920387429e-10), ('subsample', 0.5148603857041089)])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_omunRdPe4D"
      },
      "source": [
        "# train the model with tuned hyperparameters\n",
        "final_params={'objective': 'binary:logistic', 'n_estimators': 18, 'colsample_bytree': 0.6162570141151325, 'learning-rate': 0.020729429554412183,'max_depth': 7, 'min_num_child': 14, 'reg_alpha': 1.1046982314481511e-07, 'reg_lambda': 1.551926920387429e-10 , 'subsample': 0.5148603857041089}\n",
        "dtrain = xgb.DMatrix(X_train_de, y_train)\n",
        "model_xgb_de_num_cat = xgb.train(final_params, dtrain = dtrain)\n"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKzkuKfMPe4I"
      },
      "source": [
        "# testing the trained classifier\n",
        "dtest = xgb.DMatrix(X_test_de, y_test)\n",
        "y_pred = model_xgb_de_num_cat.predict(dtest)\n",
        "# turn the probabilities into labels\n",
        "y_pred[y_pred >= 0.5] = 1\n",
        "y_pred[y_pred < 0.5] = 0"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RxArv5mPe4N",
        "outputId": "e8f76602-baf8-4f17-f501-f635a502cced",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"Macro F1-Score\")\n",
        "print(f1_score(y_test, y_pred, average = \"macro\"))\n",
        "print()\n",
        "print(\"F1-score for each label\")\n",
        "print()\n",
        "print(\" ALL       AML\")\n",
        "print(f1_score(y_test, y_pred, average = None))\n"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Macro F1-Score\n",
            "0.7235772357723578\n",
            "\n",
            "F1-score for each label\n",
            "\n",
            " ALL       AML\n",
            "[0.7804878  0.66666667]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqjyKkdFQVAz"
      },
      "source": [
        "As one can see the XGBoost classifier trained only with the numerical and categorical features corresponding to DE genes performs worse, compared to the XGBoost and SVM algorithms trained on the PCA features and the categorical variables.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkWYQ-LcSR7I"
      },
      "source": [
        "Nevertheless, it is worth experimenting further to see whether we can get a reduced number of features and an on-par or better performance with the best classifiers so far. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ana2ZUuTS8pm"
      },
      "source": [
        "### **Fitting a Model Only with the Numerical Features Corresponding to DE Genes**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_-Jps9ETFX1",
        "outputId": "ac6348cb-64fa-460e-dd8e-c66c0086a6ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import skopt \n",
        "from skopt import BayesSearchCV\n",
        "import xgboost as xgb\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', message='The objective has been evaluated at this point before.')\n",
        "\n",
        "parameters = {'n_estimators': skopt.space.Integer(0,100),\n",
        "        'min_num_child': skopt.space.Integer(0, 50,),\n",
        "        'max_depth': skopt.space.Integer(0, 10),\n",
        "        'subsample': skopt.space.Real(0.5, 1.0),\n",
        "        'colsample_bytree': skopt.space.Real(0.5, 1.0),\n",
        "        'reg_lambda': skopt.space.Real(1e-10,100,'log-uniform'),\n",
        "        'reg_alpha': skopt.space.Real(1e-10,100,'log-uniform'),\n",
        "        'learning-rate': skopt.space.Real(0.01,0.2,'log-uniform'),\n",
        "        }\n",
        "\n",
        "bayes = BayesSearchCV(xgb.XGBClassifier(objective = 'binary:logistic'), search_spaces= parameters, n_iter=10, scoring='f1_macro',cv=5,random_state=0)\n",
        "res = bayes.fit(X_train_num_de,y_train)\n",
        "print(res.best_params_)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OrderedDict([('colsample_bytree', 0.6162570141151325), ('learning-rate', 0.020729429554412183), ('max_depth', 7), ('min_num_child', 14), ('n_estimators', 18), ('reg_alpha', 1.1046982314481511e-07), ('reg_lambda', 1.551926920387429e-10), ('subsample', 0.5148603857041089)])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qukZLENNTFX8"
      },
      "source": [
        "final_params={'objective': 'binary:logistic', 'n_estimators': 18, 'colsample_bytree': 0.6162570141151325, 'learning-rate': 0.020729429554412183,'max_depth': 7, 'min_num_child': 14, 'reg_alpha': 1.1046982314481511e-07, 'reg_lambda': 1.551926920387429e-10 , 'subsample': 0.5148603857041089}\n",
        "dtrain = xgb.DMatrix(X_train_num_de, y_train)\n",
        "model_xgb = xgb.train(final_params, dtrain = dtrain)\n"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCMIBkz0TFYA"
      },
      "source": [
        "dtest = xgb.DMatrix(X_test_num_de, y_test)\n",
        "y_pred = model_xgb.predict(dtest)\n",
        "# turn the probabilities into labels\n",
        "y_pred[y_pred >= 0.5] = 1\n",
        "y_pred[y_pred < 0.5] = 0"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jo_O6xw5TFYD",
        "outputId": "d83b8185-de87-429d-dc87-17f588e03d2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"Macro F1-Score\")\n",
        "print(f1_score(y_test, y_pred, average = \"macro\"))\n",
        "print()\n",
        "print(\"F1-score for each label\")\n",
        "print()\n",
        "print(\" ALL       AML\")\n",
        "print(f1_score(y_test, y_pred, average = None))\n"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Macro F1-Score\n",
            "0.7984189723320159\n",
            "\n",
            "F1-score for each label\n",
            "\n",
            " ALL       AML\n",
            "[0.86956522 0.72727273]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1lwXKhTUQdu"
      },
      "source": [
        "This classifier performs overall reasonably well on the test set, however there is a significant difference in the performance on the ALL and AML samples, when looked at separately. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3rEjtzqKrIm"
      },
      "source": [
        "### **Fitting a Model Only with the Categorical Features Corresponding to DE Genes**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sa6jmbkTEogy",
        "outputId": "b17193db-4363-457e-9df8-54c7ed539189",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import skopt \n",
        "from skopt import BayesSearchCV\n",
        "import xgboost as xgb\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', message='The objective has been evaluated at this point before.')\n",
        "\n",
        "parameters = {'n_estimators': skopt.space.Integer(0,100),\n",
        "        'min_num_child': skopt.space.Integer(0, 50,),\n",
        "        'max_depth': skopt.space.Integer(0, 10),\n",
        "        'subsample': skopt.space.Real(0.5, 1.0),\n",
        "        'colsample_bytree': skopt.space.Real(0.5, 1.0),\n",
        "        'reg_lambda': skopt.space.Real(1e-10,100,'log-uniform'),\n",
        "        'reg_alpha': skopt.space.Real(1e-10,100,'log-uniform'),\n",
        "        'learning-rate': skopt.space.Real(0.01,0.2,'log-uniform'),\n",
        "        }\n",
        "\n",
        "bayes = BayesSearchCV(xgb.XGBClassifier(objective = 'binary:logistic'), search_spaces= parameters, n_iter=10, scoring='f1_macro',cv=5,random_state=0)\n",
        "res = bayes.fit(X_train_de,y_train)\n",
        "print(res.best_params_)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OrderedDict([('colsample_bytree', 0.5737099059755859), ('learning-rate', 0.10448475115917014), ('max_depth', 5), ('min_num_child', 23), ('n_estimators', 82), ('reg_alpha', 0.0139529144831671), ('reg_lambda', 1.4176986367221247), ('subsample', 0.6597195969050774)])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9EoEfqjE58e"
      },
      "source": [
        "final_params={'objective': 'binary:logistic', 'n_estimators': 82, 'colsample_bytree': 0.5737099059755859, 'learning-rate': 0.10448475115917014,'max_depth': 5, 'min_num_child': 23, 'reg_alpha': 0.0139529144831671, 'reg_lambda': 1.4176986367221247 , 'subsample': 0.6597195969050774}\n",
        "dtrain = xgb.DMatrix(X_train_de, y_train)\n",
        "model_xgb_cat_de = xgb.train(final_params, dtrain = dtrain)\n"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODggn7nJE58m"
      },
      "source": [
        "dtest = xgb.DMatrix(X_test_de, y_test)\n",
        "y_pred = model_xgb_cat_de.predict(dtest)\n",
        "# turn the probabilities into labels\n",
        "y_pred[y_pred >= 0.5] = 1\n",
        "y_pred[y_pred < 0.5] = 0"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqKk8s-FE58q",
        "outputId": "4d8936d0-0b9a-4ae9-973c-7b7e74f37e00",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"Macro F1-Score\")\n",
        "print(f1_score(y_test, y_pred, average = \"macro\"))\n",
        "print()\n",
        "print(\"F1-score for each label\")\n",
        "print()\n",
        "print(\" ALL       AML\")\n",
        "print(f1_score(y_test, y_pred, average = None))\n"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Macro F1-Score\n",
            "0.8754578754578755\n",
            "\n",
            "F1-score for each label\n",
            "\n",
            " ALL       AML\n",
            "[0.9047619  0.84615385]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIoYSTLZZao0"
      },
      "source": [
        "As one can see, this classifier performs slightly better than the classifier trained with both the PCA numerical and the categorical features corresponding to the DE genes.\n",
        "However, there is one thing that should be considered when thinking of only using the categorical variables. We are not very clear on what the threshold is for determining the expression of a gene as present (\"P\"), absent (\"A\"), or marginal (\"M\"). And it is not very clear how robust this categorization is."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36dMxbYnDwJY"
      },
      "source": [
        "## **Comparison of All Classifiers**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1eC36V5DX2i"
      },
      "source": [
        "For convenience, here is a table of the performance results of all classifiers trained in this project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jr-ZYxj6jPd7"
      },
      "source": [
        "| | Algorithm          | Features | Num Features | Macro F1-score |ALL F1-score | AML F1-Score |\n",
        "|--|--------------------|----------|----------|---------|----------|----------|\n",
        "|1|SVM                 | PCA Num  | 30       | **0.798**  | 0.870   | 0.727  |\n",
        "|2|XGBoost             | PCA Num  | 30       | **0.807**  | 0.864   | 0.750   |\n",
        "|3|SVM                 | PCA Num + DE Cat   | 40   | **0.798**  | 0.870   | 0.727|\n",
        "|4|XGBoost             | PCA Num + DE Cat   | 40   | **0.842**  | 0.884  | 0.800|\n",
        "|5|XGBoost             | DE Num + DE Cat   | 20   | **0.724**  | 0.780  | 0.667|\n",
        "|6|XGBoost             | DE Num | 10   | **0.798** | 0.870  | 0.727   |\n",
        "|7|XGBoost             | DE Cat| 10   | **0.875**  | 0.905   | 0.846 | "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUpknt_WnjgN"
      },
      "source": [
        "Looking at the table, one can see that the top two classifiers, in terms of macro F1-score calculated on the test set, are Model 4 and Model 7.\\\n",
        "Model 4 is the XGBoost classifier, which was trained on the PCA numerical features and the categorical features, corresponding to the top 10 DE genes in the train set.\n",
        "Model 7 is the XGBoost classifier which was trained only using the categorical features corresponding to the top 10 DE genes of the train set.\\\n",
        "Of these two top models, Model 7 has slightly better macro F1-score. However, since we have only trained it on one test set, we are not sure if this difference is statistically significant, or simply due to chance.\\\n",
        "Moreover, it is usually practice to choose the simpler model, in case of two models which perform equally well on a test set. In this case, however, we do not have much information about the thresholds used by the microarray technology which determines whether a gene's expression is present (\"P\"),\n",
        "absent (\"A\"), or marginal (\"M\"). Thus, it is not very clear how robust this categorization is, making it potentially unreasonable to drop all the numerical features and relying only on the categorical ones.\\\n",
        "Thus, before choosing the simpler model with determination, it might be a good idea to consult an expert on the microarray categorization robustness first.\\\n",
        "Nevertheless, both models perform reasonably well.\n",
        "And what is more, both models appear to be doing quite well in predicting ALL and AML samples, when looked at separately, which was one of the main objectives we defined for our model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lelNItYbQEl"
      },
      "source": [
        "## **Saving the Best Models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-pKB_IdgNka"
      },
      "source": [
        "# save the best performing models\n",
        "import pickle\n",
        "# saving the model with PCA numerical features and categorical features \n",
        "# correspoinding to top 10 DE genes of the train set\n",
        "filename = 'xgboost_model_pca_num_cat_de.sav'\n",
        "pickle.dump(model_xgb_pca_num_cat_de, open(filename, 'wb'))\n",
        "\n",
        "# saving the model with only categorical features of the top 10 DE genes in the train set\n",
        "filename = 'xgboost_model_cat_de.sav'\n",
        "pickle.dump(model_xgb_cat_de, open(filename, 'wb'))"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MU1kQMvd5vq6"
      },
      "source": [
        "**REFERENCES:**\n",
        "\n",
        "[1] *Golub, T. et al, 1999. Molecular Classification of Cancer: Class Discovery and Class Prediction by Gene Expression Monitoring. Science, 286(5439), pp.531-537.*\n",
        "\n",
        "[2] Brownlee, J., 2020. How To Use Data Scaling Improve Deep Learning Model Stability And Performance. [online] Machine Learning Mastery. Available at: <https://machinelearningmastery.com/how-to-improve-neural-network-stability-and-modeling-performance-with-data-scaling/> [Accessed 10 November 2020]."
      ]
    }
  ]
}